{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Miniconda3\\envs\\llm\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "f:\\Miniconda3\\envs\\llm\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "from load_model import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone 模型\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = '../models/internlm2-chat-1_8b'\n",
    "# os.system(f'git clone https://code.openxlab.org.cn/OpenLMLab/internlm2-chat-1.8b {PRETRAINED_MODEL_NAME_OR_PATH}')\n",
    "# os.system(f'cd {PRETRAINED_MODEL_NAME_OR_PATH} && git lfs pull')\n",
    "ADAPTER_PATH = None\n",
    "# 量化\n",
    "LOAD_IN_8BIT= False\n",
    "LOAD_IN_4BIT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:  2.2.2+cu121\n",
      "transformers version:  4.36.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2bc228728d42a593a0f697e4e08f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.device: cuda:0, model.dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = load_model(PRETRAINED_MODEL_NAME_OR_PATH, ADAPTER_PATH, LOAD_IN_8BIT, LOAD_IN_4BIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternLM2TokenizerFast(name_or_path='../models/internlm2-chat-1_8b', vocab_size=92544, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|action_start|>', '<|action_end|>', '<|interpreter|>', '<|plugin|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92538: AddedToken(\"<|plugin|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92539: AddedToken(\"<|interpreter|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92540: AddedToken(\"<|action_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92541: AddedToken(\"<|action_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92542: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92543: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "[92543, 92542]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token_id)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.convert_tokens_to_ids([\"<|im_start|>\", \"<|im_end|>\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternLM2ForCausalLM(\n",
       "  (model): InternLM2Model(\n",
       "    (tok_embeddings): Embedding(92544, 2048, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x InternLM2DecoderLayer(\n",
       "        (attention): InternLM2Attention(\n",
       "          (wqkv): Linear(in_features=2048, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): InternLM2RotaryEmbedding()\n",
       "        )\n",
       "        (feed_forward): InternLM2MLP(\n",
       "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (attention_norm): InternLM2RMSNorm()\n",
       "        (ffn_norm): InternLM2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): InternLM2RMSNorm()\n",
       "  )\n",
       "  (output): Linear(in_features=2048, out_features=92544, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant whose name is InternLM (书生·浦语).\n",
    "    - InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.\n",
    "    - InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on InternLM2TokenizerFast in module transformers_modules.internlm2-chat-1_8b.tokenization_internlm2_fast object:\n",
      "\n",
      "class InternLM2TokenizerFast(transformers.tokenization_utils_fast.PreTrainedTokenizerFast)\n",
      " |  InternLM2TokenizerFast(vocab_file, unk_token='<unk>', bos_token='<s>', eos_token='</s>', pad_token='</s>', sp_model_kwargs: Optional[Dict[str, Any]] = None, add_bos_token=True, add_eos_token=False, decode_with_prefix_space=False, clean_up_tokenization_spaces=False, **kwargs)\n",
      " |  \n",
      " |  # Modified from transformers.model.llama.tokenization_llama_fast.LlamaTokenizerFast -> InternLM2TokenizerFast\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      InternLM2TokenizerFast\n",
      " |      transformers.tokenization_utils_fast.PreTrainedTokenizerFast\n",
      " |      transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
      " |      transformers.tokenization_utils_base.SpecialTokensMixin\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, vocab_file, unk_token='<unk>', bos_token='<s>', eos_token='</s>', pad_token='</s>', sp_model_kwargs: Optional[Dict[str, Any]] = None, add_bos_token=True, add_eos_token=False, decode_with_prefix_space=False, clean_up_tokenization_spaces=False, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] = None) -> Tuple[str]\n",
      " |      Save only the vocabulary of the tokenizer (vocabulary + added tokens).\n",
      " |      \n",
      " |      This method won't save the configuration and special token mappings of the tokenizer. Use\n",
      " |      [`~PreTrainedTokenizerFast._save_pretrained`] to save the whole state of the tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str`):\n",
      " |              The directory in which to save the vocabulary.\n",
      " |          filename_prefix (`str`, *optional*):\n",
      " |              An optional prefix to add to the named of the saved files.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Tuple(str)`: Paths to the files saved.\n",
      " |  \n",
      " |  update_post_processor(self)\n",
      " |      Updates the underlying post processor with the current `bos_token` and `eos_token`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  can_save_slow_tokenizer\n",
      " |      `bool`: Whether or not the slow tokenizer can be saved. Usually for sentencepiece based slow tokenizer, this\n",
      " |      can only be `True` if the original `\"sentencepiece.model\"` was not deleted.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  add_bos_token\n",
      " |  \n",
      " |  add_eos_token\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  model_input_names = ['input_ids', 'attention_mask']\n",
      " |  \n",
      " |  padding_side = 'left'\n",
      " |  \n",
      " |  slow_tokenizer_class = <class 'transformers_modules.internlm2-chat-1_8...\n",
      " |      Construct a InternLM2 tokenizer. Based on byte-level Byte-Pair-Encoding.\n",
      " |      \n",
      " |      Args:\n",
      " |          vocab_file (`str`):\n",
      " |              Path to the vocabulary file.\n",
      " |  \n",
      " |  \n",
      " |  vocab_files_names = {'vocab_file': './tokenizer.model'}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
      " |  \n",
      " |  __len__(self) -> int\n",
      " |      Size of the full vocabulary with the added tokens.\n",
      " |  \n",
      " |  convert_ids_to_tokens(self, ids: Union[int, List[int]], skip_special_tokens: bool = False) -> Union[str, List[str]]\n",
      " |      Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n",
      " |      added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (`int` or `List[int]`):\n",
      " |              The token id (or token ids) to convert to tokens.\n",
      " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str` or `List[str]`: The decoded token(s).\n",
      " |  \n",
      " |  convert_tokens_to_ids(self, tokens: Union[str, List[str]]) -> Union[int, List[int]]\n",
      " |      Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n",
      " |      vocabulary.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int` or `List[int]`: The token id or list of token ids.\n",
      " |  \n",
      " |  convert_tokens_to_string(self, tokens: List[str]) -> str\n",
      " |      Converts a sequence of tokens in a single string. The most simple way to do it is `\" \".join(tokens)` but we\n",
      " |      often want to remove sub-word tokenization artifacts at the same time.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens (`List[str]`): The token to join in a string.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: The joined tokens.\n",
      " |  \n",
      " |  get_added_vocab(self) -> Dict[str, int]\n",
      " |      Returns the added tokens in the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, int]`: The added tokens.\n",
      " |  \n",
      " |  get_vocab(self) -> Dict[str, int]\n",
      " |      Returns the vocabulary as a dictionary of token to index.\n",
      " |      \n",
      " |      `tokenizer.get_vocab()[token]` is equivalent to `tokenizer.convert_tokens_to_ids(token)` when `token` is in the\n",
      " |      vocab.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, int]`: The vocabulary.\n",
      " |  \n",
      " |  num_special_tokens_to_add(self, pair: bool = False) -> int\n",
      " |      Returns the number of added tokens when encoding a sequence with special tokens.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not put\n",
      " |      this inside your training loop.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          pair (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether the number of added tokens should be computed in the case of a sequence pair or a single\n",
      " |              sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: Number of special tokens added to sequences.\n",
      " |  \n",
      " |  set_truncation_and_padding(self, padding_strategy: transformers.utils.generic.PaddingStrategy, truncation_strategy: transformers.tokenization_utils_base.TruncationStrategy, max_length: int, stride: int, pad_to_multiple_of: Optional[int])\n",
      " |      Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers\n",
      " |      library) and restore the tokenizer settings afterwards.\n",
      " |      \n",
      " |      The provided tokenizer has no padding / truncation strategy before the managed section. If your tokenizer set a\n",
      " |      padding / truncation strategy before, then it will be reset to no padding / truncation when exiting the managed\n",
      " |      section.\n",
      " |      \n",
      " |      Args:\n",
      " |          padding_strategy ([`~utils.PaddingStrategy`]):\n",
      " |              The kind of padding that will be applied to the input\n",
      " |          truncation_strategy ([`~tokenization_utils_base.TruncationStrategy`]):\n",
      " |              The kind of truncation that will be applied to the input\n",
      " |          max_length (`int`):\n",
      " |              The maximum size of a sequence.\n",
      " |          stride (`int`):\n",
      " |              The stride to use when handling overflow.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. This is especially useful to enable\n",
      " |              the use of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).\n",
      " |  \n",
      " |  tokenize(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]\n",
      " |      Converts a string in a sequence of tokens, replacing unknown tokens with the `unk_token`.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`):\n",
      " |              The sequence to be encoded.\n",
      " |          pair (`str`, *optional*):\n",
      " |              A second sequence to be encoded with the first.\n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to add the special tokens associated with the corresponding model.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the underlying model specific encode method. See details in\n",
      " |              [`~PreTrainedTokenizerBase.__call__`]\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[str]`: The list of tokens.\n",
      " |  \n",
      " |  train_new_from_iterator(self, text_iterator, vocab_size, length=None, new_special_tokens=None, special_tokens_map=None, **kwargs)\n",
      " |      Trains a tokenizer on a new corpus with the same defaults (in terms of special tokens or tokenization pipeline)\n",
      " |      as the current one.\n",
      " |      \n",
      " |      Args:\n",
      " |          text_iterator (generator of `List[str]`):\n",
      " |              The training corpus. Should be a generator of batches of texts, for instance a list of lists of texts\n",
      " |              if you have everything in memory.\n",
      " |          vocab_size (`int`):\n",
      " |              The size of the vocabulary you want for your tokenizer.\n",
      " |          length (`int`, *optional*):\n",
      " |              The total number of sequences in the iterator. This is used to provide meaningful progress tracking\n",
      " |          new_special_tokens (list of `str` or `AddedToken`, *optional*):\n",
      " |              A list of new special tokens to add to the tokenizer you are training.\n",
      " |          special_tokens_map (`Dict[str, str]`, *optional*):\n",
      " |              If you want to rename some of the special tokens this tokenizer uses, pass along a mapping old special\n",
      " |              token name to new special token name in this argument.\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional keyword arguments passed along to the trainer from the 🤗 Tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`PreTrainedTokenizerFast`]: A new tokenizer of the same type as the original one, trained on\n",
      " |          `text_iterator`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.tokenization_utils_fast.PreTrainedTokenizerFast:\n",
      " |  \n",
      " |  added_tokens_decoder\n",
      " |      Returns the added tokens in the vocabulary as a dictionary of index to AddedToken.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Dict[str, int]`: The added tokens.\n",
      " |  \n",
      " |  added_tokens_encoder\n",
      " |      Returns the sorted mapping from string to index. The added tokens encoder is cached for performance\n",
      " |      optimisation in `self._added_tokens_encoder` for the slow tokenizers.\n",
      " |  \n",
      " |  backend_tokenizer\n",
      " |      `tokenizers.implementations.BaseTokenizer`: The Rust tokenizer used as a backend.\n",
      " |  \n",
      " |  decoder\n",
      " |      `tokenizers.decoders.Decoder`: The Rust decoder for this tokenizer.\n",
      " |  \n",
      " |  is_fast\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  vocab_size\n",
      " |      `int`: Size of the base vocabulary (without the added tokens).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, List[str], List[List[str]]] = None, text_pair: Union[str, List[str], List[List[str]], NoneType] = None, text_target: Union[str, List[str], List[List[str]]] = None, text_pair_target: Union[str, List[str], List[List[str]], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      " |      sequences.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      " |              (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      " |              `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      " |              list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      " |              you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |          text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      " |              The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      " |              list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      " |              you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
      " |              automatically.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              `>= 7.5` (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  apply_chat_template(self, conversation: Union[List[Dict[str, str]], ForwardRef('Conversation')], chat_template: Optional[str] = None, add_generation_prompt: bool = False, tokenize: bool = True, padding: bool = False, truncation: bool = False, max_length: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, **tokenizer_kwargs) -> Union[str, List[int]]\n",
      " |      Converts a Conversation object or a list of dictionaries with `\"role\"` and `\"content\"` keys to a list of token\n",
      " |      ids. This method is intended for use with chat models, and will read the tokenizer's chat_template attribute to\n",
      " |      determine the format and control tokens to use when converting. When chat_template is None, it will fall back\n",
      " |      to the default_chat_template specified at the class level.\n",
      " |      \n",
      " |      Args:\n",
      " |          conversation (Union[List[Dict[str, str]], \"Conversation\"]): A Conversation object or list of dicts\n",
      " |              with \"role\" and \"content\" keys, representing the chat history so far.\n",
      " |          chat_template (str, *optional*): A Jinja template to use for this conversion. If\n",
      " |              this is not passed, the model's default chat template will be used instead.\n",
      " |          add_generation_prompt (bool, *optional*): Whether to end the prompt with the token(s) that indicate\n",
      " |              the start of an assistant message. This is useful when you want to generate a response from the model.\n",
      " |              Note that this argument will be passed to the chat template, and so it must be supported in the\n",
      " |              template for this argument to have any effect.\n",
      " |          tokenize (`bool`, defaults to `True`):\n",
      " |              Whether to tokenize the output. If `False`, the output will be a string.\n",
      " |          padding (`bool`, defaults to `False`):\n",
      " |              Whether to pad sequences to the maximum length. Has no effect if tokenize is `False`.\n",
      " |          truncation (`bool`, defaults to `False`):\n",
      " |              Whether to truncate sequences at the maximum length. Has no effect if tokenize is `False`.\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is `False`. If\n",
      " |              not specified, the tokenizer's `max_length` attribute will be used as a default.\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors of a particular framework. Has no effect if tokenize is `False`. Acceptable\n",
      " |              values are:\n",
      " |              - `'tf'`: Return TensorFlow `tf.Tensor` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return NumPy `np.ndarray` objects.\n",
      " |              - `'jax'`: Return JAX `jnp.ndarray` objects.\n",
      " |          **tokenizer_kwargs: Additional kwargs to pass to the tokenizer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[int]`: A list of token ids representing the tokenized chat so far, including control tokens. This\n",
      " |          output is ready to pass to the model, either directly or via methods like `generate()`.\n",
      " |  \n",
      " |  as_target_tokenizer(self)\n",
      " |      Temporarily sets the tokenizer for encoding the targets. Useful for tokenizer associated to\n",
      " |      sequence-to-sequence models that need a slightly different processing for the labels.\n",
      " |  \n",
      " |  batch_decode(self, sequences: Union[List[int], List[List[int]], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> List[str]\n",
      " |      Convert a list of lists of token ids into a list of strings by calling decode.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the `__call__` method.\n",
      " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (`bool`, *optional*):\n",
      " |              Whether or not to clean up the tokenization spaces. If `None`, will default to\n",
      " |              `self.clean_up_tokenization_spaces`.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[str]`: The list of decoded sentences.\n",
      " |  \n",
      " |  batch_encode_plus(self, batch_text_or_text_pairs: Union[List[str], List[Tuple[str, str]], List[List[str]], List[Tuple[List[str], List[str]]], List[List[int]], List[Tuple[List[int], List[int]]]], add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a list of sequences or a list of pairs of sequences.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This method is deprecated, `__call__` should be used instead.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_text_or_text_pairs (`List[str]`, `List[Tuple[str, str]]`, `List[List[str]]`, `List[Tuple[List[str], List[str]]]`, and for not-fast tokenizers, also `List[List[int]]`, `List[Tuple[List[int], List[int]]]`):\n",
      " |              Batch of sequences or pair of sequences to be encoded. This can be a list of\n",
      " |              string/string-sequences/int-sequences or a list of pair of string/string-sequences/int-sequence (see\n",
      " |              details in `encode_plus`).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
      " |              automatically.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              `>= 7.5` (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  build_inputs_with_special_tokens(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]\n",
      " |      Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and\n",
      " |      adding special tokens.\n",
      " |      \n",
      " |      This implementation does not add special tokens and this method should be overridden in a subclass.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (`List[int]`): The first tokenized sequence.\n",
      " |          token_ids_1 (`List[int]`, *optional*): The second tokenized sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[int]`: The model input with special tokens.\n",
      " |  \n",
      " |  create_token_type_ids_from_sequences(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None) -> List[int]\n",
      " |      Create the token type IDs corresponding to the sequences passed. [What are token type\n",
      " |      IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |      Should be overridden in a subclass if the model has a special way of building those.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (`List[int]`): The first tokenized sequence.\n",
      " |          token_ids_1 (`List[int]`, *optional*): The second tokenized sequence.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[int]`: The token type ids.\n",
      " |  \n",
      " |  decode(self, token_ids: Union[int, List[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> str\n",
      " |      Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
      " |      tokens and clean up tokenization spaces.\n",
      " |      \n",
      " |      Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      " |              List of tokenized input ids. Can be obtained using the `__call__` method.\n",
      " |          skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to remove special tokens in the decoding.\n",
      " |          clean_up_tokenization_spaces (`bool`, *optional*):\n",
      " |              Whether or not to clean up the tokenization spaces. If `None`, will default to\n",
      " |              `self.clean_up_tokenization_spaces`.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the underlying model specific decode method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: The decoded sentence.\n",
      " |  \n",
      " |  encode(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, **kwargs) -> List[int]\n",
      " |      Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
      " |      \n",
      " |      Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`, `List[str]` or `List[int]`):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |          text_pair (`str`, `List[str]` or `List[int]`, *optional*):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
      " |              automatically.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              `>= 7.5` (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          **kwargs: Passed along to the `.tokenize()` method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `List[int]`, `torch.Tensor`, `tf.Tensor` or `np.ndarray`: The tokenized ids of the text.\n",
      " |  \n",
      " |  encode_plus(self, text: Union[str, List[str], List[int]], text_pair: Union[str, List[str], List[int], NoneType] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, is_split_into_words: bool = False, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Tokenize and prepare for the model a sequence or a pair of sequences.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This method is deprecated, `__call__` should be used instead.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          text (`str`, `List[str]` or `List[int]` (the latter only for not-fast tokenizers)):\n",
      " |              The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n",
      " |              `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |          text_pair (`str`, `List[str]` or `List[int]`, *optional*):\n",
      " |              Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
      " |              the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
      " |              method).\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
      " |              automatically.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              `>= 7.5` (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  get_special_tokens_mask(self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False) -> List[int]\n",
      " |      Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
      " |      special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n",
      " |      \n",
      " |      Args:\n",
      " |          token_ids_0 (`List[int]`):\n",
      " |              List of ids of the first sequence.\n",
      " |          token_ids_1 (`List[int]`, *optional*):\n",
      " |              List of ids of the second sequence.\n",
      " |          already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the token list is already formatted with special tokens for the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
      " |  \n",
      " |  pad(self, encoded_inputs: Union[transformers.tokenization_utils_base.BatchEncoding, List[transformers.tokenization_utils_base.BatchEncoding], Dict[str, List[int]], Dict[str, List[List[int]]], List[Dict[str, List[int]]]], padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = True, max_length: Optional[int] = None, pad_to_multiple_of: Optional[int] = None, return_attention_mask: Optional[bool] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, verbose: bool = True) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n",
      " |      in the batch.\n",
      " |      \n",
      " |      Padding side (left/right) padding token ids are defined at the tokenizer level (with `self.padding_side`,\n",
      " |      `self.pad_token_id` and `self.pad_token_type_id`).\n",
      " |      \n",
      " |      Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the\n",
      " |      text followed by a call to the `pad` method to get a padded encoding.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      If the `encoded_inputs` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the\n",
      " |      result will use the same type unless you provide a different tensor type with `return_tensors`. In the case of\n",
      " |      PyTorch tensors, you will lose the specific device of your tensors however.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          encoded_inputs ([`BatchEncoding`], list of [`BatchEncoding`], `Dict[str, List[int]]`, `Dict[str, List[List[int]]` or `List[Dict[str, List[int]]]`):\n",
      " |              Tokenized inputs. Can represent one input ([`BatchEncoding`] or `Dict[str, List[int]]`) or a batch of\n",
      " |              tokenized inputs (list of [`BatchEncoding`], *Dict[str, List[List[int]]]* or *List[Dict[str,\n",
      " |              List[int]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader\n",
      " |              collate function.\n",
      " |      \n",
      " |              Instead of `List[int]` you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors), see\n",
      " |              the note above for the return type.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
      " |               Select a strategy to pad the returned sequences (according to the model's padding side and padding\n",
      " |               index) among:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Maximum length of the returned list and optionally padding length (see above).\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value.\n",
      " |      \n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              `>= 7.5` (Volta).\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |  \n",
      " |  prepare_for_model(self, ids: List[int], pair_ids: Optional[List[int]] = None, add_special_tokens: bool = True, padding: Union[bool, str, transformers.utils.generic.PaddingStrategy] = False, truncation: Union[bool, str, transformers.tokenization_utils_base.TruncationStrategy] = None, max_length: Optional[int] = None, stride: int = 0, pad_to_multiple_of: Optional[int] = None, return_tensors: Union[str, transformers.utils.generic.TensorType, NoneType] = None, return_token_type_ids: Optional[bool] = None, return_attention_mask: Optional[bool] = None, return_overflowing_tokens: bool = False, return_special_tokens_mask: bool = False, return_offsets_mapping: bool = False, return_length: bool = False, verbose: bool = True, prepend_batch_axis: bool = False, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It\n",
      " |      adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n",
      " |      manages a moving window (with user defined stride) for overflowing tokens. Please Note, for *pair_ids*\n",
      " |      different than `None` and *truncation_strategy = longest_first* or `True`, it is not possible to return\n",
      " |      overflowing tokens. Such a combination of arguments will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and\n",
      " |              `convert_tokens_to_ids` methods.\n",
      " |          pair_ids (`List[int]`, *optional*):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`\n",
      " |              and `convert_tokens_to_ids` methods.\n",
      " |      \n",
      " |          add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      " |              `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      " |              automatically added to the input ids. This is usefull if you want to add `bos` or `eos` tokens\n",
      " |              automatically.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length to use by one of the truncation/padding parameters.\n",
      " |      \n",
      " |              If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      " |              is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      " |              length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a number along with `max_length`, the overflowing tokens returned when\n",
      " |              `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      " |              returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      " |              argument defines the number of overlapping tokens.\n",
      " |          is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      " |              tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      " |              which it will tokenize. This is useful for NER or token classification.\n",
      " |          pad_to_multiple_of (`int`, *optional*):\n",
      " |              If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      " |              This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      " |              `>= 7.5` (Volta).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |      \n",
      " |          return_token_type_ids (`bool`, *optional*):\n",
      " |              Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      " |              the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are token type IDs?](../glossary#token-type-ids)\n",
      " |          return_attention_mask (`bool`, *optional*):\n",
      " |              Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      " |              to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      " |      \n",
      " |              [What are attention masks?](../glossary#attention-mask)\n",
      " |          return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      " |              of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      " |              of returning overflowing tokens.\n",
      " |          return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return special tokens mask information.\n",
      " |          return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return `(char_start, char_end)` for each token.\n",
      " |      \n",
      " |              This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      " |              Python's tokenizer, this method will raise `NotImplementedError`.\n",
      " |          return_length  (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to return the lengths of the encoded inputs.\n",
      " |          verbose (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to print more information and warnings.\n",
      " |          **kwargs: passed to the `self.tokenize()` method\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to a model.\n",
      " |      \n",
      " |            [What are input IDs?](../glossary#input-ids)\n",
      " |      \n",
      " |          - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      " |            if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are token type IDs?](../glossary#token-type-ids)\n",
      " |      \n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      " |            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      " |      \n",
      " |            [What are attention masks?](../glossary#attention-mask)\n",
      " |      \n",
      " |          - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      " |            `return_overflowing_tokens=True`).\n",
      " |          - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      " |            regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      " |          - **length** -- The length of the inputs (when `return_length=True`)\n",
      " |  \n",
      " |  prepare_seq2seq_batch(self, src_texts: List[str], tgt_texts: Optional[List[str]] = None, max_length: Optional[int] = None, max_target_length: Optional[int] = None, padding: str = 'longest', return_tensors: str = None, truncation: bool = True, **kwargs) -> transformers.tokenization_utils_base.BatchEncoding\n",
      " |      Prepare model inputs for translation. For best performance, translate one sentence at a time.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          src_texts (`List[str]`):\n",
      " |              List of documents to summarize or source language texts.\n",
      " |          tgt_texts (`list`, *optional*):\n",
      " |              List of summaries or target language texts.\n",
      " |          max_length (`int`, *optional*):\n",
      " |              Controls the maximum length for encoder inputs (documents to summarize or source language texts) If\n",
      " |              left unset or set to `None`, this will use the predefined model maximum length if a maximum length is\n",
      " |              required by one of the truncation/padding parameters. If the model has no specific maximum input length\n",
      " |              (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      " |          max_target_length (`int`, *optional*):\n",
      " |              Controls the maximum length of decoder inputs (target language texts or summaries) If left unset or set\n",
      " |              to `None`, this will use the max_length value.\n",
      " |          padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      " |              Activates and controls padding. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      " |                sequence if provided).\n",
      " |              - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      " |                acceptable input length for the model if that argument is not provided.\n",
      " |              - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      " |                lengths).\n",
      " |          return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      " |              If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      " |      \n",
      " |              - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      " |              - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      " |              - `'np'`: Return Numpy `np.ndarray` objects.\n",
      " |          truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `True`):\n",
      " |              Activates and controls truncation. Accepts the following values:\n",
      " |      \n",
      " |              - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      " |                to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      " |                truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      " |                sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      " |                greater than the model maximum admissible input size).\n",
      " |          **kwargs:\n",
      " |              Additional keyword arguments passed along to `self.__call__`.\n",
      " |      \n",
      " |      Return:\n",
      " |          [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      " |      \n",
      " |          - **input_ids** -- List of token ids to be fed to the encoder.\n",
      " |          - **attention_mask** -- List of indices specifying which tokens should be attended to by the model.\n",
      " |          - **labels** -- List of token ids for tgt_texts.\n",
      " |      \n",
      " |          The full set of keys `[input_ids, attention_mask, labels]`, will only be returned if tgt_texts is passed.\n",
      " |          Otherwise, input_ids, attention_mask will be the only keys.\n",
      " |  \n",
      " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: str = None, commit_description: str = None, **deprecated_kwargs) -> str\n",
      " |      Upload the tokenizer files to the 🤗 Model Hub.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your tokenizer to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload tokenizer\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether or not the repository created should be private.\n",
      " |          token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
      " |              Google Colab instances without any CPU OOM issues.\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      " |          revision (`str`, *optional*):\n",
      " |              Branch to push the uploaded files to.\n",
      " |          commit_description (`str`, *optional*):\n",
      " |              The description of the commit that will be created\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      from transformers import AutoTokenizer\n",
      " |      \n",
      " |      tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      \n",
      " |      # Push the tokenizer to your namespace with the name \"my-finetuned-bert\".\n",
      " |      tokenizer.push_to_hub(\"my-finetuned-bert\")\n",
      " |      \n",
      " |      # Push the tokenizer to an organization with the name \"my-finetuned-bert\".\n",
      " |      tokenizer.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |  \n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], legacy_format: Optional[bool] = None, filename_prefix: Optional[str] = None, push_to_hub: bool = False, **kwargs) -> Tuple[str]\n",
      " |      Save the full tokenizer state.\n",
      " |      \n",
      " |      \n",
      " |      This method make sure the full tokenizer can then be re-loaded using the\n",
      " |      [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`] class method..\n",
      " |      \n",
      " |      Warning,None This won't save modifications you may have applied to the tokenizer after the instantiation (for\n",
      " |      instance, modifying `tokenizer.do_lower_case` after creation).\n",
      " |      \n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`): The path to a directory where the tokenizer will be saved.\n",
      " |          legacy_format (`bool`, *optional*):\n",
      " |              Only applicable for a fast tokenizer. If unset (default), will save the tokenizer in the unified JSON\n",
      " |              format as well as in legacy format if it exists, i.e. with tokenizer specific vocabulary and a separate\n",
      " |              added_tokens files.\n",
      " |      \n",
      " |              If `False`, will only save the tokenizer in the unified JSON format. This format is incompatible with\n",
      " |              \"slow\" tokenizers (not powered by the *tokenizers* library), so the tokenizer will not be able to be\n",
      " |              loaded in the corresponding \"slow\" tokenizer.\n",
      " |      \n",
      " |              If `True`, will save the tokenizer in legacy format. If the \"slow\" tokenizer doesn't exits, a value\n",
      " |              error is raised.\n",
      " |          filename_prefix (`str`, *optional*):\n",
      " |              A prefix to add to the names of the files saved by the tokenizer.\n",
      " |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
      " |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
      " |              namespace).\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tuple of `str`: The files saved.\n",
      " |  \n",
      " |  truncate_sequences(self, ids: List[int], pair_ids: Optional[List[int]] = None, num_tokens_to_remove: int = 0, truncation_strategy: Union[str, transformers.tokenization_utils_base.TruncationStrategy] = 'longest_first', stride: int = 0) -> Tuple[List[int], List[int], List[int]]\n",
      " |      Truncates a sequence pair in-place following the strategy.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (`List[int]`):\n",
      " |              Tokenized input ids of the first sequence. Can be obtained from a string by chaining the `tokenize` and\n",
      " |              `convert_tokens_to_ids` methods.\n",
      " |          pair_ids (`List[int]`, *optional*):\n",
      " |              Tokenized input ids of the second sequence. Can be obtained from a string by chaining the `tokenize`\n",
      " |              and `convert_tokens_to_ids` methods.\n",
      " |          num_tokens_to_remove (`int`, *optional*, defaults to 0):\n",
      " |              Number of tokens to remove using the truncation strategy.\n",
      " |          truncation_strategy (`str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      " |              The strategy to follow for truncation. Can be:\n",
      " |      \n",
      " |              - `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will truncate\n",
      " |                token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a\n",
      " |                batch of pairs) is provided.\n",
      " |              - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      " |                maximum acceptable input length for the model if that argument is not provided. This will only\n",
      " |                truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      " |              - `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths greater\n",
      " |                than the model maximum admissible input size).\n",
      " |          stride (`int`, *optional*, defaults to 0):\n",
      " |              If set to a positive number, the overflowing tokens returned will contain some tokens from the main\n",
      " |              sequence returned. The value of this argument defines the number of additional tokens.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `Tuple[List[int], List[int], List[int]]`: The truncated `ids`, the truncated `pair_ids` and the list of\n",
      " |          overflowing tokens. Note: The *longest_first* strategy returns empty list of overflowing tokens if a pair\n",
      " |          of sequences (or a batch of pairs) is provided.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  convert_added_tokens(obj: Union[tokenizers.AddedToken, Any], save=False, add_type_field=True) from builtins.type\n",
      " |  \n",
      " |  from_pretrained(pretrained_model_name_or_path: Union[str, os.PathLike], *init_inputs, cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', **kwargs) from builtins.type\n",
      " |      Instantiate a [`~tokenization_utils_base.PreTrainedTokenizerBase`] (or a derived class) from a predefined\n",
      " |      tokenizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      " |              Can be either:\n",
      " |      \n",
      " |              - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.\n",
      " |                Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n",
      " |                user or organization name, like `dbmdz/bert-base-german-cased`.\n",
      " |              - A path to a *directory* containing vocabulary files required by the tokenizer, for instance saved\n",
      " |                using the [`~tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`] method, e.g.,\n",
      " |                `./my_model_directory/`.\n",
      " |              - (**Deprecated**, not applicable to all derived classes) A path or url to a single saved vocabulary\n",
      " |                file (if and only if the tokenizer only requires a single vocabulary file like Bert or XLNet), e.g.,\n",
      " |                `./my_model_directory/vocab.txt`.\n",
      " |          cache_dir (`str` or `os.PathLike`, *optional*):\n",
      " |              Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n",
      " |              standard cache should not be used.\n",
      " |          force_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n",
      " |              exist.\n",
      " |          resume_download (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to delete incompletely received files. Attempt to resume the download if such a file\n",
      " |              exists.\n",
      " |          proxies (`Dict[str, str]`, *optional*):\n",
      " |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      " |              'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      " |          token (`str` or *bool*, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      " |          local_files_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to only rely on local files and not to attempt to download any files.\n",
      " |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      " |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      " |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      " |              identifier allowed by git.\n",
      " |          subfolder (`str`, *optional*):\n",
      " |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for\n",
      " |              facebook/rag-token-base), specify it here.\n",
      " |          inputs (additional positional arguments, *optional*):\n",
      " |              Will be passed along to the Tokenizer `__init__` method.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Will be passed to the Tokenizer `__init__` method. Can be used to set special tokens like `bos_token`,\n",
      " |              `eos_token`, `unk_token`, `sep_token`, `pad_token`, `cls_token`, `mask_token`,\n",
      " |              `additional_special_tokens`. See parameters in the `__init__` for more details.\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      Passing `token=True` is required when you want to use a private model.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # We can't instantiate directly the base class *PreTrainedTokenizerBase* so let's show our examples on a derived class: BertTokenizer\n",
      " |      # Download vocabulary from huggingface.co and cache.\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      " |      \n",
      " |      # Download vocabulary from huggingface.co (user-uploaded) and cache.\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
      " |      \n",
      " |      # If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/\")\n",
      " |      \n",
      " |      # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"./test/saved_model/my_vocab.txt\")\n",
      " |      \n",
      " |      # You can link tokens to special vocabulary when instantiating\n",
      " |      tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", unk_token=\"<unk>\")\n",
      " |      # You should be sure '<unk>' is in the vocabulary when doing that.\n",
      " |      # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
      " |      assert tokenizer.unk_token == \"<unk>\"\n",
      " |      ```\n",
      " |  \n",
      " |  register_for_auto_class(auto_class='AutoTokenizer') from builtins.type\n",
      " |      Register this class with a given auto class. This should only be used for custom tokenizers as the ones in the\n",
      " |      library are already mapped with `AutoTokenizer`.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      This API is experimental and may have some slight breaking changes in the next releases.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          auto_class (`str` or `type`, *optional*, defaults to `\"AutoTokenizer\"`):\n",
      " |              The auto class to register this new tokenizer with.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  clean_up_tokenization(out_string: str) -> str\n",
      " |      Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.\n",
      " |      \n",
      " |      Args:\n",
      " |          out_string (`str`): The text to clean up.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `str`: The cleaned-up string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  default_chat_template\n",
      " |      This template formats inputs in the standard ChatML format. See\n",
      " |      https://github.com/openai/openai-python/blob/main/chatml.md\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  max_len_sentences_pair\n",
      " |      `int`: The maximum combined length of a pair of sentences that can be fed to the model.\n",
      " |  \n",
      " |  max_len_single_sentence\n",
      " |      `int`: The maximum length of a sentence that can be fed to the model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.PreTrainedTokenizerBase:\n",
      " |  \n",
      " |  max_model_input_sizes = {}\n",
      " |  \n",
      " |  pretrained_init_configuration = {}\n",
      " |  \n",
      " |  pretrained_vocab_files_map = {}\n",
      " |  \n",
      " |  truncation_side = 'right'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  add_special_tokens(self, special_tokens_dict: Dict[str, Union[str, tokenizers.AddedToken]], replace_additional_special_tokens=True) -> int\n",
      " |      Add a dictionary of special tokens (eos, pad, cls, etc.) to the encoder and link them to class attributes. If\n",
      " |      special tokens are NOT in the vocabulary, they are added to it (indexed starting from the last index of the\n",
      " |      current vocabulary).\n",
      " |      \n",
      " |      When adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix of the\n",
      " |      model so that its embedding matrix matches the tokenizer.\n",
      " |      \n",
      " |      In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
      " |      \n",
      " |      Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n",
      " |      \n",
      " |      - Special tokens can be skipped when decoding using `skip_special_tokens = True`.\n",
      " |      - Special tokens are carefully handled by the tokenizer (they are never split), similar to `AddedTokens`.\n",
      " |      - You can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This\n",
      " |        makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
      " |      \n",
      " |      When possible, special tokens are already registered for provided pretrained models (for instance\n",
      " |      [`BertTokenizer`] `cls_token` is already registered to be :obj*'[CLS]'* and XLM's one is also registered to be\n",
      " |      `'</s>'`).\n",
      " |      \n",
      " |      Args:\n",
      " |          special_tokens_dict (dictionary *str* to *str* or `tokenizers.AddedToken`):\n",
      " |              Keys should be in the list of predefined special attributes: [`bos_token`, `eos_token`, `unk_token`,\n",
      " |              `sep_token`, `pad_token`, `cls_token`, `mask_token`, `additional_special_tokens`].\n",
      " |      \n",
      " |              Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer\n",
      " |              assign the index of the `unk_token` to them).\n",
      " |          replace_additional_special_tokens (`bool`, *optional*,, defaults to `True`):\n",
      " |              If `True`, the existing list of additional special tokens will be replaced by the list provided in\n",
      " |              `special_tokens_dict`. Otherwise, `self._additional_special_tokens` is just extended. In the former\n",
      " |              case, the tokens will NOT be removed from the tokenizer's full vocabulary - they are only being flagged\n",
      " |              as non-special tokens. Remember, this only affects which tokens are skipped during decoding, not the\n",
      " |              `added_tokens_encoder` and `added_tokens_decoder`. This means that the previous\n",
      " |              `additional_special_tokens` are still added tokens, and will not be split by the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Let's see how to add a new classification token to GPT-2\n",
      " |      tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
      " |      model = GPT2Model.from_pretrained(\"gpt2\")\n",
      " |      \n",
      " |      special_tokens_dict = {\"cls_token\": \"<CLS>\"}\n",
      " |      \n",
      " |      num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
      " |      print(\"We have added\", num_added_toks, \"tokens\")\n",
      " |      # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |      model.resize_token_embeddings(len(tokenizer))\n",
      " |      \n",
      " |      assert tokenizer.cls_token == \"<CLS>\"\n",
      " |      ```\n",
      " |  \n",
      " |  add_tokens(self, new_tokens: Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]], special_tokens: bool = False) -> int\n",
      " |      Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to\n",
      " |      it with indices starting from length of the current vocabulary and and will be isolated before the tokenization\n",
      " |      algorithm is applied. Added tokens and tokens from the vocabulary of the tokenization algorithm are therefore\n",
      " |      not treated in the same way.\n",
      " |      \n",
      " |      Note, when adding new tokens to the vocabulary, you should make sure to also resize the token embedding matrix\n",
      " |      of the model so that its embedding matrix matches the tokenizer.\n",
      " |      \n",
      " |      In order to do that, please use the [`~PreTrainedModel.resize_token_embeddings`] method.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_tokens (`str`, `tokenizers.AddedToken` or a list of *str* or `tokenizers.AddedToken`):\n",
      " |              Tokens are only added if they are not already in the vocabulary. `tokenizers.AddedToken` wraps a string\n",
      " |              token to let you personalize its behavior: whether this token should only match against a single word,\n",
      " |              whether this token should strip all potential whitespaces on the left side, whether this token should\n",
      " |              strip all potential whitespaces on the right side, etc.\n",
      " |          special_tokens (`bool`, *optional*, defaults to `False`):\n",
      " |              Can be used to specify if the token is a special token. This mostly change the normalization behavior\n",
      " |              (special tokens like CLS or [MASK] are usually not lower-cased for instance).\n",
      " |      \n",
      " |              See details for `tokenizers.AddedToken` in HuggingFace tokenizers library.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: Number of tokens added to the vocabulary.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      ```python\n",
      " |      # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
      " |      tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
      " |      model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
      " |      \n",
      " |      num_added_toks = tokenizer.add_tokens([\"new_tok1\", \"my_new-tok2\"])\n",
      " |      print(\"We have added\", num_added_toks, \"tokens\")\n",
      " |      # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e., the length of the tokenizer.\n",
      " |      model.resize_token_embeddings(len(tokenizer))\n",
      " |      ```\n",
      " |  \n",
      " |  sanitize_special_tokens(self) -> int\n",
      " |      The `sanitize_special_tokens` is now deprecated kept for backward compatibility and will be removed in\n",
      " |      transformers v5.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  all_special_ids\n",
      " |      `List[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\n",
      " |  \n",
      " |  all_special_tokens\n",
      " |      `List[str]`: A list of the unique special tokens (`'<unk>'`, `'<cls>'`, ..., etc.).\n",
      " |      \n",
      " |      Convert tokens of `tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  all_special_tokens_extended\n",
      " |      `List[Union[str, tokenizers.AddedToken]]`: All the special tokens (`'<unk>'`, `'<cls>'`, etc.), the order has\n",
      " |      nothing to do with the index of each tokens. If you want to know the correct indices, check\n",
      " |      `self.added_tokens_encoder`. We can't create an order anymore as the keys are `AddedTokens` and not `Strings`.\n",
      " |      \n",
      " |      Don't convert tokens of `tokenizers.AddedToken` type to string so they can be used to control more finely how\n",
      " |      special tokens are tokenized.\n",
      " |  \n",
      " |  pad_token_type_id\n",
      " |      `int`: Id of the padding token type in the vocabulary.\n",
      " |  \n",
      " |  special_tokens_map\n",
      " |      `Dict[str, Union[str, List[str]]]`: A dictionary mapping special token class attributes (`cls_token`,\n",
      " |      `unk_token`, etc.) to their values (`'<unk>'`, `'<cls>'`, etc.).\n",
      " |      \n",
      " |      Convert potential tokens of `tokenizers.AddedToken` type to string.\n",
      " |  \n",
      " |  special_tokens_map_extended\n",
      " |      `Dict[str, Union[str, tokenizers.AddedToken, List[Union[str, tokenizers.AddedToken]]]]`: A dictionary mapping\n",
      " |      special token class attributes (`cls_token`, `unk_token`, etc.) to their values (`'<unk>'`, `'<cls>'`, etc.).\n",
      " |      \n",
      " |      Don't convert tokens of `tokenizers.AddedToken` type to string so they can be used to control more finely how\n",
      " |      special tokens are tokenized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  additional_special_tokens\n",
      " |      `List[str]`: All the additional special tokens you may want to use. Log an error if used while not having been\n",
      " |      set.\n",
      " |  \n",
      " |  additional_special_tokens_ids\n",
      " |      `List[int]`: Ids of all the additional special tokens in the vocabulary. Log an error if used while not having\n",
      " |      been set.\n",
      " |  \n",
      " |  bos_token\n",
      " |      `str`: Beginning of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  bos_token_id\n",
      " |      `Optional[int]`: Id of the beginning of sentence token in the vocabulary. Returns `None` if the token has not\n",
      " |      been set.\n",
      " |  \n",
      " |  cls_token\n",
      " |      `str`: Classification token, to extract a summary of an input sequence leveraging self-attention along the full\n",
      " |      depth of the model. Log an error if used while not having been set.\n",
      " |  \n",
      " |  cls_token_id\n",
      " |      `Optional[int]`: Id of the classification token in the vocabulary, to extract a summary of an input sequence\n",
      " |      leveraging self-attention along the full depth of the model.\n",
      " |      \n",
      " |      Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  eos_token\n",
      " |      `str`: End of sentence token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  eos_token_id\n",
      " |      `Optional[int]`: Id of the end of sentence token in the vocabulary. Returns `None` if the token has not been\n",
      " |      set.\n",
      " |  \n",
      " |  mask_token\n",
      " |      `str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while not\n",
      " |      having been set.\n",
      " |  \n",
      " |  mask_token_id\n",
      " |      `Optional[int]`: Id of the mask token in the vocabulary, used when training a model with masked-language\n",
      " |      modeling. Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  pad_token\n",
      " |      `str`: Padding token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  pad_token_id\n",
      " |      `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  sep_token\n",
      " |      `str`: Separation token, to separate context and query in an input sequence. Log an error if used while not\n",
      " |      having been set.\n",
      " |  \n",
      " |  sep_token_id\n",
      " |      `Optional[int]`: Id of the separation token in the vocabulary, to separate context and query in an input\n",
      " |      sequence. Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  unk_token\n",
      " |      `str`: Unknown token. Log an error if used while not having been set.\n",
      " |  \n",
      " |  unk_token_id\n",
      " |      `Optional[int]`: Id of the unknown token in the vocabulary. Returns `None` if the token has not been set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from transformers.tokenization_utils_base.SpecialTokensMixin:\n",
      " |  \n",
      " |  SPECIAL_TOKENS_ATTRIBUTES = ['bos_token', 'eos_token', 'unk_token', 's...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/internlm/internlm2-chat-1_8b/blob/main/modeling_internlm2.py#L1136\n",
    "def build_inputs(tokenizer, query: str, history: list[tuple[str, str]] = [], meta_instruction=\"\"):\n",
    "    if tokenizer.add_bos_token:\n",
    "        prompt = \"\"\n",
    "    else:\n",
    "        prompt = tokenizer.bos_token\n",
    "    if meta_instruction:\n",
    "        prompt += f\"\"\"<|im_start|>system\\n{meta_instruction}<|im_end|>\\n\"\"\"\n",
    "    for record in history:\n",
    "        prompt += f\"\"\"<|im_start|>user\\n{record[0]}<|im_end|>\\n<|im_start|>assistant\\n{record[1]}<|im_end|>\\n\"\"\"\n",
    "    prompt += f\"\"\"<|im_start|>user\\n{query}<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
    "    return prompt, tokenizer([prompt], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an AI assistant whose name is InternLM (书生·浦语).\n",
      "    - InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.\n",
      "    - InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.\n",
      "    <|im_end|>\n",
      "<|im_start|>user\n",
      "给我讲一个猫和老鼠的小故事<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt, inputs = build_inputs(tokenizer, \"给我讲一个猫和老鼠的小故事\", history=[], meta_instruction=SYSTEM_PROMPT)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  tensor([[    1, 92543,  9081,   364,  2770,   657,   589, 15358, 17993,  6843,\n",
      "           963,   505,  4576, 11146,   451, 60628, 60384, 60721, 62442, 60752,\n",
      "          4452,   388,   285,  4576, 11146,   451, 60628, 60384, 60721, 62442,\n",
      "         60752,   313,   505,   395,  7659,  1813,  4287,  1762,   560,   505,\n",
      "          8020,   684, 36956, 15358, 31288,   451, 68589, 76659, 71581,   699,\n",
      "          1226,   505,  6342,   442,   517, 11100,   328, 10894,   328,   454,\n",
      "         51978,   756,   388,   285,  4576, 11146,   451, 60628, 60384, 60721,\n",
      "         62442, 60752,   313,   777,  3696,   454, 19187, 19829,  4563,   435,\n",
      "           410,  4287, 12032,   684,   410,  1341,  1893,   569,  6519,   454,\n",
      "           262, 69093,   756,   388, 92542,   364, 92543,  1008,   364, 68706,\n",
      "         61077, 68252, 61519, 60381, 74262, 68447, 68654, 92542,   364, 92543,\n",
      "           525, 11353,   364]], device='cuda:0')\n",
      "attention_mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.to(model.device)\n",
    "print(\"input_ids: \", inputs[\"input_ids\"]) # 开始为1,添加了 bos_token_id\n",
    "print(\"attention_mask: \", inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    2,\n",
       "    92542\n",
       "  ],\n",
       "  \"max_new_tokens\": 1024,\n",
       "  \"temperature\": 0.8,\n",
       "  \"top_k\": 40,\n",
       "  \"top_p\": 0.8\n",
       "}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens = 1024,\n",
    "    do_sample = True,\n",
    "    num_beams = 1,\n",
    "    temperature = 0.8,\n",
    "    top_k = 40,\n",
    "    top_p = 0.8,\n",
    "    eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids([\"<|im_end|>\"])[0]]\n",
    ")\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 92543,  9081,   364,  2770,   657,   589, 15358, 17993,  6843,\n",
       "           963,   505,  4576, 11146,   451, 60628, 60384, 60721, 62442, 60752,\n",
       "          4452,   388,   285,  4576, 11146,   451, 60628, 60384, 60721, 62442,\n",
       "         60752,   313,   505,   395,  7659,  1813,  4287,  1762,   560,   505,\n",
       "          8020,   684, 36956, 15358, 31288,   451, 68589, 76659, 71581,   699,\n",
       "          1226,   505,  6342,   442,   517, 11100,   328, 10894,   328,   454,\n",
       "         51978,   756,   388,   285,  4576, 11146,   451, 60628, 60384, 60721,\n",
       "         62442, 60752,   313,   777,  3696,   454, 19187, 19829,  4563,   435,\n",
       "           410,  4287, 12032,   684,   410,  1341,  1893,   569,  6519,   454,\n",
       "           262, 69093,   756,   388, 92542,   364, 92543,  1008,   364, 68706,\n",
       "         61077, 68252, 61519, 60381, 74262, 68447, 68654, 92542,   364, 92543,\n",
       "           525, 11353,   364, 68529, 68251, 60355, 68262, 68654, 73126,   904,\n",
       "         69253, 69368, 70511,   338,  1467,  1320,   281, 60354, 70249, 76044,\n",
       "         60503, 61519, 60381, 74262, 60501, 60355, 68654, 92155, 61519, 60381,\n",
       "         74262, 69713, 73459, 73859, 60353, 68375, 68310, 68343, 68705, 69702,\n",
       "         76543, 71775, 80092, 60355,   402, 61519, 78131, 60419, 61310, 61279,\n",
       "         60420, 60353, 74262, 78131, 60419, 64917, 64917, 60420, 60355, 68310,\n",
       "         74360, 72951, 61653, 64068, 60380, 60353, 61310, 61279, 68556, 61882,\n",
       "         60633, 64917, 64917, 60354, 73503, 60353, 60458, 64917, 64917, 60395,\n",
       "         68965, 74375, 72477, 61310, 61279, 60355, 68310, 69713, 70696, 68705,\n",
       "         69085, 60353, 69125, 70619, 60353, 61310, 61279, 68792, 70714, 69786,\n",
       "         60355,   402, 61310, 61279, 60379, 60404, 60462, 62900, 72607, 61310,\n",
       "         61279, 60361, 78559, 60797, 68849, 70329, 60419, 61310, 61279, 60420,\n",
       "         60353, 60367, 60602, 75824, 60355, 64917, 64917, 68335, 70974, 60353,\n",
       "         68792, 73838, 68530, 68262, 69864, 60355, 60404, 60379, 60404, 60462,\n",
       "         61867, 72607, 71123, 62589, 83550, 61310, 61279, 60354, 61310, 61279,\n",
       "         60353, 68265, 61931, 68384, 68457, 61786, 60425, 60355,   402, 61310,\n",
       "         61279, 69836, 68335, 72415, 60381, 75493, 60353, 60404, 68792, 60691,\n",
       "         64917, 64917, 78889, 60355, 60404, 68301, 75517, 64917, 64917, 60353,\n",
       "         68614, 68556, 79539, 61786, 60542, 69512, 69335, 74934, 60355, 64917,\n",
       "         64917, 69836, 68335, 69944, 60353, 60404, 68301, 86684, 61663, 60709,\n",
       "         60661, 60353, 74375, 77692, 61310, 61279, 60354, 80092, 60355,   402,\n",
       "         61310, 61279, 68528, 75517, 64917, 64917, 60353, 60573, 68705, 72163,\n",
       "         69409, 70063, 68304, 81252, 60355, 60404, 68566, 68301, 60379, 68457,\n",
       "         61310, 61279, 61726, 64199, 64917, 64917, 60354, 74047, 60353, 74375,\n",
       "         68988, 68457, 68272, 60355, 64917, 64917, 69836, 68335, 70054, 60381,\n",
       "         74346, 60353, 60404, 76740, 68329, 68630, 79867, 61310, 61279, 60354,\n",
       "         61286, 62333, 60355,   402, 68985, 60353, 61310, 61279, 68354, 68849,\n",
       "         83893, 60353, 60404, 62662, 76667, 60353, 76669, 60415, 69322, 60486,\n",
       "         64917, 64917, 60354, 73623, 60355, 64917, 64917, 69426, 60549, 61310,\n",
       "         61279, 60354, 61310, 61279, 60431, 69467, 60353, 60404, 60825, 71860,\n",
       "         83893, 60353, 60510, 61310, 61279, 70428, 80861, 86860, 80092, 60811,\n",
       "         60355,   402, 60361, 70793, 86860, 80092, 60366, 60353, 61310, 61279,\n",
       "         60379, 68457, 61310, 61279, 69055, 60362, 64917, 64917, 60353, 60499,\n",
       "         64917, 64917, 61032, 81478, 60415, 62662, 70653, 68374, 69055, 60355,\n",
       "         68985, 60353, 61310, 61279, 61603, 70896, 64917, 64917, 60353, 60404,\n",
       "         72260, 69365, 68304, 83893, 60353, 60461, 69899, 68345, 61128, 60563,\n",
       "         71632, 60355,   402, 68262, 68654, 80749, 60353, 69195, 69204, 71237,\n",
       "         60381, 60442, 64952, 65828, 60354, 70712, 60353, 68579, 68253, 68638,\n",
       "         74106, 60381, 88339, 60353, 68253, 69558, 75229, 69290, 60355, 92542]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids = inputs[\"input_ids\"],\n",
    "        attention_mask = inputs[\"attention_mask\"],\n",
    "        generation_config = generation_config,\n",
    "    )\n",
    "# 输出是二维的\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([68529, 68251, 60355, 68262, 68654, 73126,   904, 69253, 69368, 70511,\n",
       "          338,  1467,  1320,   281, 60354, 70249, 76044, 60503, 61519, 60381,\n",
       "        74262, 60501, 60355, 68654, 92155, 61519, 60381, 74262, 69713, 73459,\n",
       "        73859, 60353, 68375, 68310, 68343, 68705, 69702, 76543, 71775, 80092,\n",
       "        60355,   402, 61519, 78131, 60419, 61310, 61279, 60420, 60353, 74262,\n",
       "        78131, 60419, 64917, 64917, 60420, 60355, 68310, 74360, 72951, 61653,\n",
       "        64068, 60380, 60353, 61310, 61279, 68556, 61882, 60633, 64917, 64917,\n",
       "        60354, 73503, 60353, 60458, 64917, 64917, 60395, 68965, 74375, 72477,\n",
       "        61310, 61279, 60355, 68310, 69713, 70696, 68705, 69085, 60353, 69125,\n",
       "        70619, 60353, 61310, 61279, 68792, 70714, 69786, 60355,   402, 61310,\n",
       "        61279, 60379, 60404, 60462, 62900, 72607, 61310, 61279, 60361, 78559,\n",
       "        60797, 68849, 70329, 60419, 61310, 61279, 60420, 60353, 60367, 60602,\n",
       "        75824, 60355, 64917, 64917, 68335, 70974, 60353, 68792, 73838, 68530,\n",
       "        68262, 69864, 60355, 60404, 60379, 60404, 60462, 61867, 72607, 71123,\n",
       "        62589, 83550, 61310, 61279, 60354, 61310, 61279, 60353, 68265, 61931,\n",
       "        68384, 68457, 61786, 60425, 60355,   402, 61310, 61279, 69836, 68335,\n",
       "        72415, 60381, 75493, 60353, 60404, 68792, 60691, 64917, 64917, 78889,\n",
       "        60355, 60404, 68301, 75517, 64917, 64917, 60353, 68614, 68556, 79539,\n",
       "        61786, 60542, 69512, 69335, 74934, 60355, 64917, 64917, 69836, 68335,\n",
       "        69944, 60353, 60404, 68301, 86684, 61663, 60709, 60661, 60353, 74375,\n",
       "        77692, 61310, 61279, 60354, 80092, 60355,   402, 61310, 61279, 68528,\n",
       "        75517, 64917, 64917, 60353, 60573, 68705, 72163, 69409, 70063, 68304,\n",
       "        81252, 60355, 60404, 68566, 68301, 60379, 68457, 61310, 61279, 61726,\n",
       "        64199, 64917, 64917, 60354, 74047, 60353, 74375, 68988, 68457, 68272,\n",
       "        60355, 64917, 64917, 69836, 68335, 70054, 60381, 74346, 60353, 60404,\n",
       "        76740, 68329, 68630, 79867, 61310, 61279, 60354, 61286, 62333, 60355,\n",
       "          402, 68985, 60353, 61310, 61279, 68354, 68849, 83893, 60353, 60404,\n",
       "        62662, 76667, 60353, 76669, 60415, 69322, 60486, 64917, 64917, 60354,\n",
       "        73623, 60355, 64917, 64917, 69426, 60549, 61310, 61279, 60354, 61310,\n",
       "        61279, 60431, 69467, 60353, 60404, 60825, 71860, 83893, 60353, 60510,\n",
       "        61310, 61279, 70428, 80861, 86860, 80092, 60811, 60355,   402, 60361,\n",
       "        70793, 86860, 80092, 60366, 60353, 61310, 61279, 60379, 68457, 61310,\n",
       "        61279, 69055, 60362, 64917, 64917, 60353, 60499, 64917, 64917, 61032,\n",
       "        81478, 60415, 62662, 70653, 68374, 69055, 60355, 68985, 60353, 61310,\n",
       "        61279, 61603, 70896, 64917, 64917, 60353, 60404, 72260, 69365, 68304,\n",
       "        83893, 60353, 60461, 69899, 68345, 61128, 60563, 71632, 60355,   402,\n",
       "        68262, 68654, 80749, 60353, 69195, 69204, 71237, 60381, 60442, 64952,\n",
       "        65828, 60354, 70712, 60353, 68579, 68253, 68638, 74106, 60381, 88339,\n",
       "        60353, 68253, 69558, 75229, 69290, 60355, 92542])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取出第一条数据\n",
    "ids = outputs[0].cpu()[len(inputs[\"input_ids\"][0]) :]\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以。这个故事源于19世纪英国作家J.M.W.的著名童话《猫和老鼠》。故事描述了猫和老鼠之间的激烈斗争，以及他们如何不断尝试逃避对方的追逐。\n",
      "\n",
      "猫名叫“胡须”，老鼠名叫“吱吱”。他们住在同一个屋檐下，胡须经常偷吃吱吱的粮食，而吱吱也总是试图抓住胡须。他们之间的矛盾不断升级，直到有一天，胡须决定采取行动。\n",
      "\n",
      "胡须用他那锐利的胡须在墙上画了一个很大的“胡须”，以示警告。吱吱非常生气，决定亲自解决这个麻烦。他用他那尖利的牙齿咬破了胡须的胡须，然后逃到了他的洞里。\n",
      "\n",
      "胡须感到非常失望和愤怒，他决定向吱吱复仇。他开始跟踪吱吱，并且经常在他的洞口周围制造噪音。吱吱感到非常害怕，他开始躲在墙角处，试图躲避胡须的追逐。\n",
      "\n",
      "胡须继续跟踪吱吱，并不断在他身上留下自己的印记。他甚至开始用他的胡须抓挠吱吱的尾巴，试图引起他的注意。吱吱感到非常痛苦和绝望，他知道自己已经无法逃脱胡须的追捕。\n",
      "\n",
      "最终，胡须找到了一个洞穴，他躲在那里，静静地等待着吱吱的到来。吱吱终于被胡须的胡须所吸引，他走进了洞穴，与胡须展开了一场激烈的追逐战。\n",
      "\n",
      "在一场激烈的追逐中，胡须用他的胡须攻击了吱吱，但吱吱却巧妙地躲开了这些攻击。最终，胡须败给了吱吱，他不得不离开自己的洞穴，去寻找其他藏身之处。\n",
      "\n",
      "这个故事告诉我们，即使是最强大和最狡猾的对手，只要我们保持冷静和机智，我们也能战胜它们。\n"
     ]
    }
   ],
   "source": [
    "# decode 处理一维数据\n",
    "response = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以。这个故事源于19世纪英国作家J.M.W.的著名童话《猫和老鼠》。故事描述了猫和老鼠之间的激烈斗争，以及他们如何不断尝试逃避对方的追逐。\n",
      "\n",
      "猫名叫“胡须”，老鼠名叫“吱吱”。他们住在同一个屋檐下，胡须经常偷吃吱吱的粮食，而吱吱也总是试图抓住胡须。他们之间的矛盾不断升级，直到有一天，胡须决定采取行动。\n",
      "\n",
      "胡须用他那锐利的胡须在墙上画了一个很大的“胡须”，以示警告。吱吱非常生气，决定亲自解决这个麻烦。他用他那尖利的牙齿咬破了胡须的胡须，然后逃到了他的洞里。\n",
      "\n",
      "胡须感到非常失望和愤怒，他决定向吱吱复仇。他开始跟踪吱吱，并且经常在他的洞口周围制造噪音。吱吱感到非常害怕，他开始躲在墙角处，试图躲避胡须的追逐。\n",
      "\n",
      "胡须继续跟踪吱吱，并不断在他身上留下自己的印记。他甚至开始用他的胡须抓挠吱吱的尾巴，试图引起他的注意。吱吱感到非常痛苦和绝望，他知道自己已经无法逃脱胡须的追捕。\n",
      "\n",
      "最终，胡须找到了一个洞穴，他躲在那里，静静地等待着吱吱的到来。吱吱终于被胡须的胡须所吸引，他走进了洞穴，与胡须展开了一场激烈的追逐战。\n",
      "\n",
      "在一场激烈的追逐中，胡须用他的胡须攻击了吱吱，但吱吱却巧妙地躲开了这些攻击。最终，胡须败给了吱吱，他不得不离开自己的洞穴，去寻找其他藏身之处。\n",
      "\n",
      "这个故事告诉我们，即使是最强大和最狡猾的对手，只要我们保持冷静和机智，我们也能战胜它们。\n"
     ]
    }
   ],
   "source": [
    "# batch_decode 处理二维数据\n",
    "print(tokenizer.batch_decode([ids], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
