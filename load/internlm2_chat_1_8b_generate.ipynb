{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "from load_model import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone 模型\n",
    "PRETRAINED_MODEL_NAME_OR_PATH = '../models/internlm2-chat-1_8b'\n",
    "# os.system(f'git clone https://code.openxlab.org.cn/OpenLMLab/internlm2-chat-1.8b {PRETRAINED_MODEL_NAME_OR_PATH}')\n",
    "# os.system(f'cd {PRETRAINED_MODEL_NAME_OR_PATH} && git lfs pull')\n",
    "ADAPTER_PATH = None\n",
    "# 量化\n",
    "LOAD_IN_8BIT= False\n",
    "LOAD_IN_4BIT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:  2.3.0+cu121\n",
      "transformers version:  4.40.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba3a0657f4547c49df02122d0927427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.device: cuda:0, model.dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model = load_model(PRETRAINED_MODEL_NAME_OR_PATH, ADAPTER_PATH, LOAD_IN_8BIT, LOAD_IN_4BIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternLM2TokenizerFast(name_or_path='../models/internlm2-chat-1_8b', vocab_size=92544, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|action_start|>', '<|action_end|>', '<|interpreter|>', '<|plugin|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92538: AddedToken(\"<|plugin|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92539: AddedToken(\"<|interpreter|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92540: AddedToken(\"<|action_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92541: AddedToken(\"<|action_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92542: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92543: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternLM2ForCausalLM(\n",
       "  (model): InternLM2Model(\n",
       "    (tok_embeddings): Embedding(92544, 2048, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x InternLM2DecoderLayer(\n",
       "        (attention): InternLM2Attention(\n",
       "          (wqkv): Linear(in_features=2048, out_features=4096, bias=False)\n",
       "          (wo): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): InternLM2RotaryEmbedding()\n",
       "        )\n",
       "        (feed_forward): InternLM2MLP(\n",
       "          (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (attention_norm): InternLM2RMSNorm()\n",
       "        (ffn_norm): InternLM2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): InternLM2RMSNorm()\n",
       "  )\n",
       "  (output): Linear(in_features=2048, out_features=92544, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant whose name is InternLM (书生·浦语).\n",
    "    - InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.\n",
    "    - InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(query: str, history: list[tuple[str, str]] = [], meta_instruction=\"我是系统\"):\n",
    "    prompt = \"\"\n",
    "    if meta_instruction:\n",
    "        # <s> tokenizer会默认添加,不过这里使用手动添加的方式\n",
    "        prompt += f\"\"\"<s><|im_start|>system\\n{meta_instruction}<|im_end|>\\n\"\"\"\n",
    "    else:\n",
    "        prompt += \"<s>\"\n",
    "    for record in history:\n",
    "        prompt += f\"\"\"<|im_start|>user\\n{record[0]}<|im_end|>\\n<|im_start|>assistant\\n{record[1]}<|im_end|>\\n\"\"\"\n",
    "    prompt += f\"\"\"<|im_start|>user\\n{query}<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|im_start|>system\n",
      "You are an AI assistant whose name is InternLM (书生·浦语).\n",
      "    - InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.\n",
      "    - InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.\n",
      "    <|im_end|>\n",
      "<|im_start|>user\n",
      "给我讲一个猫和老鼠的小故事<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = build_inputs(\"给我讲一个猫和老鼠的小故事\", history=[], meta_instruction=SYSTEM_PROMPT)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  tensor([[    1, 92543,  9081,   364,  2770,   657,   589, 15358, 17993,  6843,\n",
      "           963,   505,  4576, 11146,   451, 60628, 60384, 60721, 62442, 60752,\n",
      "          4452,   388,   285,  4576, 11146,   451, 60628, 60384, 60721, 62442,\n",
      "         60752,   313,   505,   395,  7659,  1813,  4287,  1762,   560,   505,\n",
      "          8020,   684, 36956, 15358, 31288,   451, 68589, 76659, 71581,   699,\n",
      "          1226,   505,  6342,   442,   517, 11100,   328, 10894,   328,   454,\n",
      "         51978,   756,   388,   285,  4576, 11146,   451, 60628, 60384, 60721,\n",
      "         62442, 60752,   313,   777,  3696,   454, 19187, 19829,  4563,   435,\n",
      "           410,  4287, 12032,   684,   410,  1341,  1893,   569,  6519,   454,\n",
      "           262, 69093,   756,   388, 92542,   364, 92543,  1008,   364, 68706,\n",
      "         61077, 68252, 61519, 60381, 74262, 68447, 68654, 92542,   364, 92543,\n",
      "           525, 11353,   364]], device='cuda:0')\n",
      "attention_mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(inputs, add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n",
    "print(\"input_ids: \", inputs[\"input_ids\"])\n",
    "print(\"attention_mask: \", inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "92542\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.convert_tokens_to_ids([\"<|im_end|>\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    2,\n",
       "    92542\n",
       "  ],\n",
       "  \"max_new_tokens\": 1024,\n",
       "  \"temperature\": 0.8,\n",
       "  \"top_k\": 40,\n",
       "  \"top_p\": 0.8\n",
       "}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens = 1024,\n",
    "    do_sample = True,\n",
    "    num_beams = 1,\n",
    "    temperature = 0.8,\n",
    "    top_k = 40,\n",
    "    top_p = 0.8,\n",
    "    eos_token_id = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids([\"<|im_end|>\"])[0]]\n",
    ")\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 92543,  9081,   364,  2770,   657,   589, 15358, 17993,  6843,\n",
       "           963,   505,  4576, 11146,   451, 60628, 60384, 60721, 62442, 60752,\n",
       "          4452,   388,   285,  4576, 11146,   451, 60628, 60384, 60721, 62442,\n",
       "         60752,   313,   505,   395,  7659,  1813,  4287,  1762,   560,   505,\n",
       "          8020,   684, 36956, 15358, 31288,   451, 68589, 76659, 71581,   699,\n",
       "          1226,   505,  6342,   442,   517, 11100,   328, 10894,   328,   454,\n",
       "         51978,   756,   388,   285,  4576, 11146,   451, 60628, 60384, 60721,\n",
       "         62442, 60752,   313,   777,  3696,   454, 19187, 19829,  4563,   435,\n",
       "           410,  4287, 12032,   684,   410,  1341,  1893,   569,  6519,   454,\n",
       "           262, 69093,   756,   388, 92542,   364, 92543,  1008,   364, 68706,\n",
       "         61077, 68252, 61519, 60381, 74262, 68447, 68654, 92542,   364, 92543,\n",
       "           525, 11353,   364, 68529, 68251, 60355, 68262, 68654, 73126,   904,\n",
       "         69253, 69368, 70511,   338,  1467,  1320,   281, 60354, 70249, 76044,\n",
       "         60503, 61519, 60381, 74262, 60501, 60355, 68654, 92155, 61519, 60381,\n",
       "         74262, 69713, 73459, 73859, 60353, 68375, 68310, 68343, 68705, 69702,\n",
       "         76543, 71775, 80092, 60355,   402, 61519, 78131, 60419, 61310, 61279,\n",
       "         60420, 60353, 74262, 78131, 60419, 64917, 64917, 60420, 60355, 68310,\n",
       "         74360, 72951, 61653, 64068, 60380, 60353, 61310, 61279, 68556, 61882,\n",
       "         60633, 64917, 64917, 60354, 73503, 60353, 60458, 64917, 64917, 60395,\n",
       "         68965, 74375, 72477, 61310, 61279, 60355, 68310, 69713, 70696, 68705,\n",
       "         69085, 60353, 69125, 70619, 60353, 61310, 61279, 68792, 70714, 69786,\n",
       "         60355,   402, 61310, 61279, 60379, 60404, 60462, 62900, 72607, 61310,\n",
       "         61279, 60361, 78559, 60797, 68849, 70329, 60419, 61310, 61279, 60420,\n",
       "         60353, 60367, 60602, 75824, 60355, 64917, 64917, 68335, 70974, 60353,\n",
       "         68792, 73838, 68530, 68262, 69864, 60355, 60404, 60379, 60404, 60462,\n",
       "         61867, 72607, 71123, 62589, 83550, 61310, 61279, 60354, 61310, 61279,\n",
       "         60353, 68265, 61931, 68384, 68457, 61786, 60425, 60355,   402, 61310,\n",
       "         61279, 69836, 68335, 72415, 60381, 75493, 60353, 60404, 68792, 60691,\n",
       "         64917, 64917, 78889, 60355, 60404, 68301, 75517, 64917, 64917, 60353,\n",
       "         68614, 68556, 79539, 61786, 60542, 69512, 69335, 74934, 60355, 64917,\n",
       "         64917, 69836, 68335, 69944, 60353, 60404, 68301, 86684, 61663, 60709,\n",
       "         60661, 60353, 74375, 77692, 61310, 61279, 60354, 80092, 60355,   402,\n",
       "         61310, 61279, 68528, 75517, 64917, 64917, 60353, 60573, 68705, 72163,\n",
       "         69409, 70063, 68304, 81252, 60355, 60404, 68566, 68301, 60379, 68457,\n",
       "         61310, 61279, 61726, 64199, 64917, 64917, 60354, 74047, 60353, 74375,\n",
       "         68988, 68457, 68272, 60355, 64917, 64917, 69836, 68335, 70054, 60381,\n",
       "         74346, 60353, 60404, 76740, 68329, 68630, 79867, 61310, 61279, 60354,\n",
       "         61286, 62333, 60355,   402, 68985, 60353, 61310, 61279, 68354, 68849,\n",
       "         83893, 60353, 60404, 62662, 76667, 60353, 76669, 60415, 69322, 60486,\n",
       "         64917, 64917, 60354, 73623, 60355, 64917, 64917, 69426, 60549, 61310,\n",
       "         61279, 60354, 61310, 61279, 60431, 69467, 60353, 60404, 60825, 71860,\n",
       "         83893, 60353, 60510, 61310, 61279, 70428, 80861, 86860, 80092, 60811,\n",
       "         60355,   402, 60361, 70793, 86860, 80092, 60366, 60353, 61310, 61279,\n",
       "         60379, 68457, 61310, 61279, 69055, 60362, 64917, 64917, 60353, 60499,\n",
       "         64917, 64917, 61032, 81478, 60415, 62662, 70653, 68374, 69055, 60355,\n",
       "         68985, 60353, 61310, 61279, 61603, 70896, 64917, 64917, 60353, 60404,\n",
       "         72260, 69365, 68304, 83893, 60353, 60461, 69899, 68345, 61128, 60563,\n",
       "         71632, 60355,   402, 68262, 68654, 80749, 60353, 69195, 69204, 71237,\n",
       "         60381, 60442, 64952, 65828, 60354, 70712, 60353, 68579, 68253, 68638,\n",
       "         74106, 60381, 88339, 60353, 68253, 69558, 75229, 69290, 60355, 92542]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids = inputs[\"input_ids\"],\n",
    "        attention_mask = inputs[\"attention_mask\"],\n",
    "        generation_config = generation_config,\n",
    "    )\n",
    "# 输出是二维的\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([68529, 68251, 60355, 68262, 68654, 73126,   904, 69253, 69368, 70511,\n",
       "          338,  1467,  1320,   281, 60354, 70249, 76044, 60503, 61519, 60381,\n",
       "        74262, 60501, 60355, 68654, 92155, 61519, 60381, 74262, 69713, 73459,\n",
       "        73859, 60353, 68375, 68310, 68343, 68705, 69702, 76543, 71775, 80092,\n",
       "        60355,   402, 61519, 78131, 60419, 61310, 61279, 60420, 60353, 74262,\n",
       "        78131, 60419, 64917, 64917, 60420, 60355, 68310, 74360, 72951, 61653,\n",
       "        64068, 60380, 60353, 61310, 61279, 68556, 61882, 60633, 64917, 64917,\n",
       "        60354, 73503, 60353, 60458, 64917, 64917, 60395, 68965, 74375, 72477,\n",
       "        61310, 61279, 60355, 68310, 69713, 70696, 68705, 69085, 60353, 69125,\n",
       "        70619, 60353, 61310, 61279, 68792, 70714, 69786, 60355,   402, 61310,\n",
       "        61279, 60379, 60404, 60462, 62900, 72607, 61310, 61279, 60361, 78559,\n",
       "        60797, 68849, 70329, 60419, 61310, 61279, 60420, 60353, 60367, 60602,\n",
       "        75824, 60355, 64917, 64917, 68335, 70974, 60353, 68792, 73838, 68530,\n",
       "        68262, 69864, 60355, 60404, 60379, 60404, 60462, 61867, 72607, 71123,\n",
       "        62589, 83550, 61310, 61279, 60354, 61310, 61279, 60353, 68265, 61931,\n",
       "        68384, 68457, 61786, 60425, 60355,   402, 61310, 61279, 69836, 68335,\n",
       "        72415, 60381, 75493, 60353, 60404, 68792, 60691, 64917, 64917, 78889,\n",
       "        60355, 60404, 68301, 75517, 64917, 64917, 60353, 68614, 68556, 79539,\n",
       "        61786, 60542, 69512, 69335, 74934, 60355, 64917, 64917, 69836, 68335,\n",
       "        69944, 60353, 60404, 68301, 86684, 61663, 60709, 60661, 60353, 74375,\n",
       "        77692, 61310, 61279, 60354, 80092, 60355,   402, 61310, 61279, 68528,\n",
       "        75517, 64917, 64917, 60353, 60573, 68705, 72163, 69409, 70063, 68304,\n",
       "        81252, 60355, 60404, 68566, 68301, 60379, 68457, 61310, 61279, 61726,\n",
       "        64199, 64917, 64917, 60354, 74047, 60353, 74375, 68988, 68457, 68272,\n",
       "        60355, 64917, 64917, 69836, 68335, 70054, 60381, 74346, 60353, 60404,\n",
       "        76740, 68329, 68630, 79867, 61310, 61279, 60354, 61286, 62333, 60355,\n",
       "          402, 68985, 60353, 61310, 61279, 68354, 68849, 83893, 60353, 60404,\n",
       "        62662, 76667, 60353, 76669, 60415, 69322, 60486, 64917, 64917, 60354,\n",
       "        73623, 60355, 64917, 64917, 69426, 60549, 61310, 61279, 60354, 61310,\n",
       "        61279, 60431, 69467, 60353, 60404, 60825, 71860, 83893, 60353, 60510,\n",
       "        61310, 61279, 70428, 80861, 86860, 80092, 60811, 60355,   402, 60361,\n",
       "        70793, 86860, 80092, 60366, 60353, 61310, 61279, 60379, 68457, 61310,\n",
       "        61279, 69055, 60362, 64917, 64917, 60353, 60499, 64917, 64917, 61032,\n",
       "        81478, 60415, 62662, 70653, 68374, 69055, 60355, 68985, 60353, 61310,\n",
       "        61279, 61603, 70896, 64917, 64917, 60353, 60404, 72260, 69365, 68304,\n",
       "        83893, 60353, 60461, 69899, 68345, 61128, 60563, 71632, 60355,   402,\n",
       "        68262, 68654, 80749, 60353, 69195, 69204, 71237, 60381, 60442, 64952,\n",
       "        65828, 60354, 70712, 60353, 68579, 68253, 68638, 74106, 60381, 88339,\n",
       "        60353, 68253, 69558, 75229, 69290, 60355, 92542])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取出第一条数据\n",
    "ids = outputs[0].cpu()[len(inputs[\"input_ids\"][0]) :]\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以。这个故事源于19世纪英国作家J.M.W.的著名童话《猫和老鼠》。故事描述了猫和老鼠之间的激烈斗争，以及他们如何不断尝试逃避对方的追逐。\n",
      "\n",
      "猫名叫“胡须”，老鼠名叫“吱吱”。他们住在同一个屋檐下，胡须经常偷吃吱吱的粮食，而吱吱也总是试图抓住胡须。他们之间的矛盾不断升级，直到有一天，胡须决定采取行动。\n",
      "\n",
      "胡须用他那锐利的胡须在墙上画了一个很大的“胡须”，以示警告。吱吱非常生气，决定亲自解决这个麻烦。他用他那尖利的牙齿咬破了胡须的胡须，然后逃到了他的洞里。\n",
      "\n",
      "胡须感到非常失望和愤怒，他决定向吱吱复仇。他开始跟踪吱吱，并且经常在他的洞口周围制造噪音。吱吱感到非常害怕，他开始躲在墙角处，试图躲避胡须的追逐。\n",
      "\n",
      "胡须继续跟踪吱吱，并不断在他身上留下自己的印记。他甚至开始用他的胡须抓挠吱吱的尾巴，试图引起他的注意。吱吱感到非常痛苦和绝望，他知道自己已经无法逃脱胡须的追捕。\n",
      "\n",
      "最终，胡须找到了一个洞穴，他躲在那里，静静地等待着吱吱的到来。吱吱终于被胡须的胡须所吸引，他走进了洞穴，与胡须展开了一场激烈的追逐战。\n",
      "\n",
      "在一场激烈的追逐中，胡须用他的胡须攻击了吱吱，但吱吱却巧妙地躲开了这些攻击。最终，胡须败给了吱吱，他不得不离开自己的洞穴，去寻找其他藏身之处。\n",
      "\n",
      "这个故事告诉我们，即使是最强大和最狡猾的对手，只要我们保持冷静和机智，我们也能战胜它们。\n"
     ]
    }
   ],
   "source": [
    "# decode 处理一维数据\n",
    "response = tokenizer.decode(ids, skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然可以。这个故事源于19世纪英国作家J.M.W.的著名童话《猫和老鼠》。故事描述了猫和老鼠之间的激烈斗争，以及他们如何不断尝试逃避对方的追逐。\n",
      "\n",
      "猫名叫“胡须”，老鼠名叫“吱吱”。他们住在同一个屋檐下，胡须经常偷吃吱吱的粮食，而吱吱也总是试图抓住胡须。他们之间的矛盾不断升级，直到有一天，胡须决定采取行动。\n",
      "\n",
      "胡须用他那锐利的胡须在墙上画了一个很大的“胡须”，以示警告。吱吱非常生气，决定亲自解决这个麻烦。他用他那尖利的牙齿咬破了胡须的胡须，然后逃到了他的洞里。\n",
      "\n",
      "胡须感到非常失望和愤怒，他决定向吱吱复仇。他开始跟踪吱吱，并且经常在他的洞口周围制造噪音。吱吱感到非常害怕，他开始躲在墙角处，试图躲避胡须的追逐。\n",
      "\n",
      "胡须继续跟踪吱吱，并不断在他身上留下自己的印记。他甚至开始用他的胡须抓挠吱吱的尾巴，试图引起他的注意。吱吱感到非常痛苦和绝望，他知道自己已经无法逃脱胡须的追捕。\n",
      "\n",
      "最终，胡须找到了一个洞穴，他躲在那里，静静地等待着吱吱的到来。吱吱终于被胡须的胡须所吸引，他走进了洞穴，与胡须展开了一场激烈的追逐战。\n",
      "\n",
      "在一场激烈的追逐中，胡须用他的胡须攻击了吱吱，但吱吱却巧妙地躲开了这些攻击。最终，胡须败给了吱吱，他不得不离开自己的洞穴，去寻找其他藏身之处。\n",
      "\n",
      "这个故事告诉我们，即使是最强大和最狡猾的对手，只要我们保持冷静和机智，我们也能战胜它们。\n"
     ]
    }
   ],
   "source": [
    "# batch_decode 处理二维数据\n",
    "print(tokenizer.batch_decode([ids], skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
