{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入必要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, BitsAndBytesConfig, GenerationConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:  2.2.1+cu121\n",
      "transformers version:  4.39.0\n"
     ]
    }
   ],
   "source": [
    "print(\"torch version: \", torch.__version__)\n",
    "print(\"transformers version: \", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512    # 分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "data_path = \"../data/huanhuan.json\"\n",
    "pretrained_model_name_or_path = \"../models/internlm2-chat-1_8b\"\n",
    "work_dir = \"../work_dirs/internlm2_chat_1_8b_qlora_huanhuan_e3_hf\"\n",
    "system_prompt = \"现在你要扮演皇帝身边的女人--甄嬛\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——</td>\n",
       "      <td></td>\n",
       "      <td>嘘——都说许愿说破是不灵的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>这个温太医啊，也是古怪，谁不知太医不得皇命不能为皇族以外的人请脉诊病，他倒好，十天半月便往咱...</td>\n",
       "      <td></td>\n",
       "      <td>你们俩话太多了，我该和温太医要一剂药，好好治治你们。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>嬛妹妹，刚刚我去府上请脉，听甄伯母说你来这里进香了。</td>\n",
       "      <td></td>\n",
       "      <td>出来走走，也是散心。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>嬛妹妹，我虽是一介御医，俸禄微薄，可是我保证会一生一世对你好，疼爱你，保护你，永远事事以你为...</td>\n",
       "      <td></td>\n",
       "      <td>实初哥哥这么说，就枉顾我们一直以来的兄妹情谊了，嬛儿没有哥哥，一直把你当作自己的亲哥哥一样看...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>实初虽然唐突了妹妹，却是真心实意地希望妹妹不要去应选，这不仅仅是因为我心里一直把妹妹当成……...</td>\n",
       "      <td></td>\n",
       "      <td>我们两家是世交，昔年恩义不过是父亲随手之劳，不必挂怀。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>你好</td>\n",
       "      <td></td>\n",
       "      <td>皇上好，我是甄嬛，家父是大理寺少卿甄远道。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>你是谁？</td>\n",
       "      <td></td>\n",
       "      <td>我是甄嬛，家父是大理寺少卿甄远道。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>你好</td>\n",
       "      <td></td>\n",
       "      <td>皇上好，我是甄嬛，家父是大理寺少卿甄远道。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>你是谁？</td>\n",
       "      <td></td>\n",
       "      <td>我是甄嬛，家父是大理寺少卿甄远道。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>你好</td>\n",
       "      <td></td>\n",
       "      <td>皇上好，我是甄嬛，家父是大理寺少卿甄远道。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3729 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction input  \\\n",
       "0                 小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——         \n",
       "1     这个温太医啊，也是古怪，谁不知太医不得皇命不能为皇族以外的人请脉诊病，他倒好，十天半月便往咱...         \n",
       "2                            嬛妹妹，刚刚我去府上请脉，听甄伯母说你来这里进香了。         \n",
       "3     嬛妹妹，我虽是一介御医，俸禄微薄，可是我保证会一生一世对你好，疼爱你，保护你，永远事事以你为...         \n",
       "4     实初虽然唐突了妹妹，却是真心实意地希望妹妹不要去应选，这不仅仅是因为我心里一直把妹妹当成……...         \n",
       "...                                                 ...   ...   \n",
       "3724                                                 你好         \n",
       "3725                                               你是谁？         \n",
       "3726                                                 你好         \n",
       "3727                                               你是谁？         \n",
       "3728                                                 你好         \n",
       "\n",
       "                                                 output  \n",
       "0                                        嘘——都说许愿说破是不灵的。  \n",
       "1                            你们俩话太多了，我该和温太医要一剂药，好好治治你们。  \n",
       "2                                            出来走走，也是散心。  \n",
       "3     实初哥哥这么说，就枉顾我们一直以来的兄妹情谊了，嬛儿没有哥哥，一直把你当作自己的亲哥哥一样看...  \n",
       "4                           我们两家是世交，昔年恩义不过是父亲随手之劳，不必挂怀。  \n",
       "...                                                 ...  \n",
       "3724                              皇上好，我是甄嬛，家父是大理寺少卿甄远道。  \n",
       "3725                                  我是甄嬛，家父是大理寺少卿甄远道。  \n",
       "3726                              皇上好，我是甄嬛，家父是大理寺少卿甄远道。  \n",
       "3727                                  我是甄嬛，家父是大理寺少卿甄远道。  \n",
       "3728                              皇上好，我是甄嬛，家父是大理寺少卿甄远道。  \n",
       "\n",
       "[3729 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用datasets读取数据\n",
    "df = pd.read_json(data_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 3729\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(df)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': '小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——',\n",
       " 'input': '',\n",
       " 'output': '嘘——都说许愿说破是不灵的。'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'output', 'instruction'],\n",
      "    num_rows: 3729\n",
      "})\n",
      "{'input': '', 'output': '嘘——都说许愿说破是不灵的。', 'instruction': '小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——'}\n"
     ]
    }
   ],
   "source": [
    "print(Dataset.from_json(data_path))\n",
    "print(Dataset.from_json(data_path)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output', 'instruction'],\n",
      "        num_rows: 3729\n",
      "    })\n",
      "})\n",
      "{'input': '', 'output': '嘘——都说许愿说破是不灵的。', 'instruction': '小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——'}\n"
     ]
    }
   ],
   "source": [
    "print(load_dataset('json', data_files=data_path))\n",
    "print(load_dataset('json', data_files=data_path)['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternLM2Tokenizer(name_or_path='../models/internlm2-chat-1_8b', vocab_size=92544, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|action_start|>', '<|action_end|>', '<|interpreter|>', '<|plugin|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92538: AddedToken(\"<|plugin|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92539: AddedToken(\"<|interpreter|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92540: AddedToken(\"<|action_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92541: AddedToken(\"<|action_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92542: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92543: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, use_fast=False, trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0, 92543, 92542, 92541, 92540, 92539, 92538]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '</s>',\n",
       " '<unk>',\n",
       " '<|im_start|>',\n",
       " '<|im_end|>',\n",
       " '<|action_start|>',\n",
       " '<|action_end|>',\n",
       " '<|interpreter|>',\n",
       " '<|plugin|>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 92543, 92542]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<s>[UNUSED_TOKEN_146][UNUSED_TOKEN_145]\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <s><|im_start|><|im_end|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1, 92543, 92542], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 92543, 92542]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<s><|im_start|><|im_end|>\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/InternLM/xtuner/blob/main/xtuner/utils/templates.py#L24\n",
    "internlm2_chat = dict(\n",
    "    SYSTEM = '<|im_start|>system\\n{system}<|im_end|>\\n',\n",
    "    INSTRUCTION = ('<|im_start|>user\\n{input}<|im_end|>\\n'\n",
    "                   '<|im_start|>assistant\\n'),\n",
    "    SUFFIX = '<|im_end|>',\n",
    "    SUFFIX_AS_EOS = True,\n",
    "    SEP = '\\n',\n",
    "    STOP_WORDS = ['<|im_end|>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/internlm/internlm2-chat-1_8b/blob/main/modeling_internlm2.py#L1136\n",
    "def build_inputs(tokenizer, query: str, history: list[tuple[str, str]] = [], meta_instruction=\"\"):\n",
    "    if tokenizer.add_bos_token:\n",
    "        prompt = \"\"\n",
    "    else:\n",
    "        prompt = tokenizer.bos_token\n",
    "    if meta_instruction:\n",
    "        prompt += f\"\"\"<|im_start|>system\\n{meta_instruction}<|im_end|>\\n\"\"\"\n",
    "    for record in history:\n",
    "        prompt += f\"\"\"<|im_start|>user\\n{record[0]}<|im_end|>\\n<|im_start|>assistant\\n{record[1]}<|im_end|>\\n\"\"\"\n",
    "    prompt += f\"\"\"<|im_start|>user\\n{query}<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
    "    return prompt, tokenizer([prompt], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s><|im_start|>system\\n我是系统<|im_end|>\\n<|im_start|>user\\n你好<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_inputs('你好')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    # print(example)\n",
    "    # {\n",
    "    #     'instruction': '小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——',\n",
    "    #     'input': '',\n",
    "    #     'output': '嘘——都说许愿说破是不灵的。'\n",
    "    # }\n",
    "\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    # <s> tokenizer会默认添加,不过这里使用手动添加的方式\n",
    "    instruction = tokenizer(f\"<s><|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{example['instruction']}<|im_end|>\\n<|im_start|>assistant\\n\", add_special_tokens=False)  # add_special_tokens 不在开头加 special_tokens\n",
    "    response = tokenizer(f\"{example['output']}<|im_end|>\\n\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.eos_token_id]             # tokenizer.eos_token_id = 2 是 </s>\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]                   # 因为eos token咱们也是要关注的所以 补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.eos_token_id]  # 3条数据长度相同\n",
    "\n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602ceab9c7b3410296760bffd9f2c2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3729 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3729\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove_columns: map 后会移除这一列\n",
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "[1, 92543, 9081, 364, 68293, 69538, 71156, 70621, 73488, 68943, 444, 63840, 65094, 92542, 364, 92543, 1008, 364, 73752, 60353, 69616, 61261, 60553, 69088, 60763, 60366, 60459, 60353, 79402, 72435, 73752, 60475, 60549, 66820, 76005, 60353, 79668, 68323, 68808, 60591, 60591, 74246, 92395, 92542, 364, 92543, 525, 11353, 364, 64389, 92395, 72598, 90192, 60423, 61198, 69153, 61255, 60354, 60355, 92542, 364, 2]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 64389, 92395, 72598, 90192, 60423, 61198, 69153, 61255, 60354, 60355, 92542, 364, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_id[0].keys())\n",
    "print(tokenized_id[0]['input_ids'])\n",
    "print(tokenized_id[0]['attention_mask'])\n",
    "print(tokenized_id[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <s><|im_start|> system\n",
      "现在你要扮演皇帝身边的女人--甄嬛<|im_end|> \n",
      "<|im_start|> user\n",
      "小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——<|im_end|> \n",
      "<|im_start|> assistant\n",
      "嘘——都说许愿说破是不灵的。<|im_end|> \n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenized_id[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 嘘——都说许愿说破是不灵的。<|im_end|> \\n</s>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[0][\"labels\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # 是否在4位精度下加载模型。如果设置为True，则在4位精度下加载模型。\n",
    "    load_in_8bit=False,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.float16,   # 4位精度计算的数据类型。这里设置为torch.float16，表示使用半精度浮点数。\n",
    "    bnb_4bit_quant_type='nf4',              # 4位精度量化的类型。这里设置为\"nf4\"，表示使用nf4量化类型。 nf4: 4bit-NormalFloat\n",
    "    bnb_4bit_use_double_quant=True,         # 是否使用双精度量化。如果设置为True，则使用双精度量化。\n",
    ")\n",
    "quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    low_cpu_mem_usage=True,             # 是否使用低CPU内存，使用 device_map 参数必须为 True\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "model.enable_input_require_grads()      # 开启梯度检查点时，要执行该方法\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"model.device: {model.device}, model.dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training, load_peft_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/peft/developer_guides/quantization\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,   # 训练模式\n",
    "    r=64,                   # Lora 秩\n",
    "    target_modules=['wqkv', 'wo', 'w1', 'w2', 'w3'],\n",
    "    lora_alpha=16,          # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1,       # Dropout 比例\n",
    "    bias='none'\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=work_dir,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=1e-5,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # 4*4=16\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",  # epoch or steps\n",
    "    save_steps=1,           # 每个epoch保存一次模型\n",
    "    save_total_limit=3,\n",
    "    save_on_each_node=True,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    bf16 = False,   # 指定训练时的类型\n",
    "    fp16 = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
