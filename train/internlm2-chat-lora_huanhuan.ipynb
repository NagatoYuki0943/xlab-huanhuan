{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入必要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, BitsAndBytesConfig, GenerationConfig\n",
    "import torch\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:  2.2.1+cu121\n",
      "transformers version:  4.38.2\n"
     ]
    }
   ],
   "source": [
    "print(\"torch version: \", torch.__version__)\n",
    "print(\"transformers version: \", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512    # 分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "data_path = \"../data/huanhuan.json\"\n",
    "model_dir = \"../models/internlm2-chat-1_8b-sft\"\n",
    "work_dir = \"../work_dirs/internlm2_lora_huanhuan\"\n",
    "system_prompt = \"现在你要扮演皇帝身边的女人--甄嬛\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——</td>\n",
       "      <td></td>\n",
       "      <td>嘘——都说许愿说破是不灵的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>这个温太医啊，也是古怪，谁不知太医不得皇命不能为皇族以外的人请脉诊病，他倒好，十天半月便往咱...</td>\n",
       "      <td></td>\n",
       "      <td>你们俩话太多了，我该和温太医要一剂药，好好治治你们。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>嬛妹妹，刚刚我去府上请脉，听甄伯母说你来这里进香了。</td>\n",
       "      <td></td>\n",
       "      <td>出来走走，也是散心。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>嬛妹妹，我虽是一介御医，俸禄微薄，可是我保证会一生一世对你好，疼爱你，保护你，永远事事以你为...</td>\n",
       "      <td></td>\n",
       "      <td>实初哥哥这么说，就枉顾我们一直以来的兄妹情谊了，嬛儿没有哥哥，一直把你当作自己的亲哥哥一样看...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>实初虽然唐突了妹妹，却是真心实意地希望妹妹不要去应选，这不仅仅是因为我心里一直把妹妹当成……...</td>\n",
       "      <td></td>\n",
       "      <td>我们两家是世交，昔年恩义不过是父亲随手之劳，不必挂怀。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>你好</td>\n",
       "      <td></td>\n",
       "      <td>皇上好，我是甄嬛，家父是大理寺少卿甄远道。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>你是谁？</td>\n",
       "      <td></td>\n",
       "      <td>我是甄嬛，家父是大理寺少卿甄远道。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>你好</td>\n",
       "      <td></td>\n",
       "      <td>皇上好，我是甄嬛，家父是大理寺少卿甄远道。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>你是谁？</td>\n",
       "      <td></td>\n",
       "      <td>我是甄嬛，家父是大理寺少卿甄远道。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>你好</td>\n",
       "      <td></td>\n",
       "      <td>皇上好，我是甄嬛，家父是大理寺少卿甄远道。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3729 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction input  \\\n",
       "0                 小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——         \n",
       "1     这个温太医啊，也是古怪，谁不知太医不得皇命不能为皇族以外的人请脉诊病，他倒好，十天半月便往咱...         \n",
       "2                            嬛妹妹，刚刚我去府上请脉，听甄伯母说你来这里进香了。         \n",
       "3     嬛妹妹，我虽是一介御医，俸禄微薄，可是我保证会一生一世对你好，疼爱你，保护你，永远事事以你为...         \n",
       "4     实初虽然唐突了妹妹，却是真心实意地希望妹妹不要去应选，这不仅仅是因为我心里一直把妹妹当成……...         \n",
       "...                                                 ...   ...   \n",
       "3724                                                 你好         \n",
       "3725                                               你是谁？         \n",
       "3726                                                 你好         \n",
       "3727                                               你是谁？         \n",
       "3728                                                 你好         \n",
       "\n",
       "                                                 output  \n",
       "0                                        嘘——都说许愿说破是不灵的。  \n",
       "1                            你们俩话太多了，我该和温太医要一剂药，好好治治你们。  \n",
       "2                                            出来走走，也是散心。  \n",
       "3     实初哥哥这么说，就枉顾我们一直以来的兄妹情谊了，嬛儿没有哥哥，一直把你当作自己的亲哥哥一样看...  \n",
       "4                           我们两家是世交，昔年恩义不过是父亲随手之劳，不必挂怀。  \n",
       "...                                                 ...  \n",
       "3724                              皇上好，我是甄嬛，家父是大理寺少卿甄远道。  \n",
       "3725                                  我是甄嬛，家父是大理寺少卿甄远道。  \n",
       "3726                              皇上好，我是甄嬛，家父是大理寺少卿甄远道。  \n",
       "3727                                  我是甄嬛，家父是大理寺少卿甄远道。  \n",
       "3728                              皇上好，我是甄嬛，家父是大理寺少卿甄远道。  \n",
       "\n",
       "[3729 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用datasets读取数据\n",
    "df = pd.read_json(data_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 3729\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Dataset.from_pandas(df)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternLM2Tokenizer(name_or_path='../models/internlm2-chat-1_8b-sft', vocab_size=92544, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|action_start|>', '<|action_end|>', '<|interpreter|>', '<|plugin|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92538: AddedToken(\"<|plugin|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92539: AddedToken(\"<|interpreter|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92540: AddedToken(\"<|action_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92541: AddedToken(\"<|action_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92542: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t92543: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(query: str, history: List[Tuple[str, str]] = [], meta_instruction=\"我是系统\"):\n",
    "    prompt = \"\"\n",
    "    if meta_instruction:\n",
    "        prompt += f\"\"\"<s>[UNUSED_TOKEN_146]system\\n{meta_instruction}[UNUSED_TOKEN_145]\\n\"\"\"\n",
    "    else:\n",
    "        prompt += \"<s>\"\n",
    "    for record in history:\n",
    "        prompt += f\"\"\"[UNUSED_TOKEN_146]user\\n{record[0]}[UNUSED_TOKEN_145]\\n[UNUSED_TOKEN_146]assistant\\n{record[1]}[UNUSED_TOKEN_145]\\n\"\"\"\n",
    "    prompt += f\"\"\"[UNUSED_TOKEN_146]user\\n{query}[UNUSED_TOKEN_145]\\n[UNUSED_TOKEN_146]assistant\\n\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[UNUSED_TOKEN_146]system\\n我是系统[UNUSED_TOKEN_145]\\n[UNUSED_TOKEN_146]user\\n你哈[UNUSED_TOKEN_145]\\n[UNUSED_TOKEN_146]assistant\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_inputs('你哈')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"<s>[UNUSED_TOKEN_146]system{system_prompt}[UNUSED_TOKEN_145]\\n[UNUSED_TOKEN_146]user{example['instruction']}[UNUSED_TOKEN_145]\\n[UNUSED_TOKEN_146]assistant\", add_special_tokens=False)  # add_special_tokens 不在开头加 special_tokens\n",
    "    response = tokenizer(f\"{example['output']}[UNUSED_TOKEN_145]<s>\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a4a9f7208648e78827807fdc7cffb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3729 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3729\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)\n",
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' <s><|im_start|> system现在你要扮演皇帝身边的女人--甄嬛<|im_end|> \\n<|im_start|> user小姐，别的秀女都在求中选，唯有咱们小姐想被撂牌子，菩萨一定记得真真儿的——<|im_end|> \\n<|im_start|> assistant嘘——都说许愿说破是不灵的。<|im_end|><s></s>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_id[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 你们俩话太多了，我该和温太医要一剂药，好好治治你们。<|im_end|><s></s>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[1][\"labels\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitsAndBytesConfig {\n",
       "  \"_load_in_4bit\": true,\n",
       "  \"_load_in_8bit\": false,\n",
       "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
       "  \"bnb_4bit_quant_type\": \"nf4\",\n",
       "  \"bnb_4bit_use_double_quant\": true,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": true,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # 是否在4位精度下加载模型。如果设置为True，则在4位精度下加载模型。\n",
    "    load_in_8bit=False,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_compute_dtype=torch.float16,   # 4位精度计算的数据类型。这里设置为torch.float16，表示使用半精度浮点数。\n",
    "    bnb_4bit_quant_type='nf4',              # 4位精度量化的类型。这里设置为\"nf4\"，表示使用nf4量化类型。 nf4: 4bit-NormalFloat\n",
    "    bnb_4bit_use_double_quant=True,         # 是否使用双精度量化。如果设置为True，则使用双精度量化。\n",
    ")\n",
    "quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternLM2ForCausalLM(\n",
       "  (model): InternLM2Model(\n",
       "    (tok_embeddings): Embedding(92544, 2048, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x InternLM2DecoderLayer(\n",
       "        (attention): InternLM2Attention(\n",
       "          (wqkv): Linear4bit(in_features=2048, out_features=4096, bias=False)\n",
       "          (wo): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): InternLM2DynamicNTKScalingRotaryEmbedding()\n",
       "        )\n",
       "        (feed_forward): InternLM2MLP(\n",
       "          (w1): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "          (w3): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "          (w2): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (attention_norm): InternLM2RMSNorm()\n",
       "        (ffn_norm): InternLM2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): InternLM2RMSNorm()\n",
       "  )\n",
       "  (output): Linear(in_features=2048, out_features=92544, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map='auto',\n",
    "    low_cpu_mem_usage=True,             # 是否使用低CPU内存，使用 device_map 参数必须为 True\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "model.enable_input_require_grads()      # 开启梯度检查点时，要执行该方法\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.device: cuda:0, model.dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(f\"model.device: {model.device}, model.dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training, load_peft_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternLM2ForCausalLM(\n",
       "  (model): InternLM2Model(\n",
       "    (tok_embeddings): Embedding(92544, 2048, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x InternLM2DecoderLayer(\n",
       "        (attention): InternLM2Attention(\n",
       "          (wqkv): Linear4bit(in_features=2048, out_features=4096, bias=False)\n",
       "          (wo): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): InternLM2DynamicNTKScalingRotaryEmbedding()\n",
       "        )\n",
       "        (feed_forward): InternLM2MLP(\n",
       "          (w1): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "          (w3): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
       "          (w2): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (attention_norm): InternLM2RMSNorm()\n",
       "        (ffn_norm): InternLM2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): InternLM2RMSNorm()\n",
       "  )\n",
       "  (output): Linear(in_features=2048, out_features=92544, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/peft/developer_guides/quantization\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'w3', 'wo', 'wqkv', 'w2', 'w1'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,   # 训练模式\n",
    "    r=64,                   # Lora 秩\n",
    "    target_modules=['wqkv', 'wo', 'w1', 'w2', 'w3'],\n",
    "    lora_alpha=16,          # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.1,       # Dropout 比例\n",
    "    bias='none'\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='../models/internlm2-chat-1_8b-sft', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'w3', 'wo', 'wqkv', 'w2', 'w1'}, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,864,320 || all params: 1,896,974,336 || trainable%: 0.41457176571944937\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=work_dir,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=1e-5,\n",
    "    gradient_checkpointing=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,  # 4*4=16\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    save_on_each_node=True,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287d4d4ed90d4d17b2cacb1daed6729c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/699 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.7562, 'grad_norm': 7.630174160003662, 'learning_rate': 9.856938483547926e-06, 'epoch': 0.04}\n",
      "{'loss': 5.2451, 'grad_norm': 10.935853004455566, 'learning_rate': 9.713876967095852e-06, 'epoch': 0.09}\n",
      "{'loss': 4.7601, 'grad_norm': 4.187314510345459, 'learning_rate': 9.570815450643777e-06, 'epoch': 0.13}\n",
      "{'loss': 4.4317, 'grad_norm': 6.8559794425964355, 'learning_rate': 9.427753934191704e-06, 'epoch': 0.17}\n",
      "{'loss': 4.1082, 'grad_norm': 7.876904010772705, 'learning_rate': 9.284692417739628e-06, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in ../models/internlm2-chat-1_8b-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7065, 'grad_norm': 4.046258926391602, 'learning_rate': 9.141630901287555e-06, 'epoch': 0.26}\n",
      "{'loss': 3.5902, 'grad_norm': 4.043776988983154, 'learning_rate': 8.99856938483548e-06, 'epoch': 0.3}\n",
      "{'loss': 3.5723, 'grad_norm': 5.262574195861816, 'learning_rate': 8.855507868383405e-06, 'epoch': 0.34}\n",
      "{'loss': 3.5481, 'grad_norm': 3.2736563682556152, 'learning_rate': 8.712446351931331e-06, 'epoch': 0.39}\n",
      "{'loss': 3.4757, 'grad_norm': 4.843493938446045, 'learning_rate': 8.569384835479256e-06, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in ../models/internlm2-chat-1_8b-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4897, 'grad_norm': 3.0951592922210693, 'learning_rate': 8.426323319027183e-06, 'epoch': 0.47}\n",
      "{'loss': 3.4594, 'grad_norm': 3.688026189804077, 'learning_rate': 8.283261802575108e-06, 'epoch': 0.51}\n",
      "{'loss': 3.4343, 'grad_norm': 3.1130030155181885, 'learning_rate': 8.140200286123034e-06, 'epoch': 0.56}\n",
      "{'loss': 3.394, 'grad_norm': 3.4914615154266357, 'learning_rate': 7.997138769670959e-06, 'epoch': 0.6}\n",
      "{'loss': 3.5113, 'grad_norm': 3.9971094131469727, 'learning_rate': 7.854077253218885e-06, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in ../models/internlm2-chat-1_8b-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4465, 'grad_norm': 3.4326534271240234, 'learning_rate': 7.71101573676681e-06, 'epoch': 0.69}\n",
      "{'loss': 3.4097, 'grad_norm': 4.076249599456787, 'learning_rate': 7.567954220314736e-06, 'epoch': 0.73}\n",
      "{'loss': 3.3085, 'grad_norm': 3.7515368461608887, 'learning_rate': 7.424892703862662e-06, 'epoch': 0.77}\n",
      "{'loss': 3.3806, 'grad_norm': 4.741869926452637, 'learning_rate': 7.281831187410587e-06, 'epoch': 0.81}\n",
      "{'loss': 3.3625, 'grad_norm': 4.607834815979004, 'learning_rate': 7.138769670958513e-06, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in ../models/internlm2-chat-1_8b-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3465, 'grad_norm': 3.4230518341064453, 'learning_rate': 6.995708154506439e-06, 'epoch': 0.9}\n",
      "{'loss': 3.3541, 'grad_norm': 4.14011287689209, 'learning_rate': 6.8526466380543645e-06, 'epoch': 0.94}\n",
      "{'loss': 3.4006, 'grad_norm': 4.135137557983398, 'learning_rate': 6.70958512160229e-06, 'epoch': 0.99}\n",
      "{'loss': 3.4529, 'grad_norm': 3.93792462348938, 'learning_rate': 6.566523605150214e-06, 'epoch': 1.03}\n",
      "{'loss': 3.3488, 'grad_norm': 4.572661399841309, 'learning_rate': 6.42346208869814e-06, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in ../models/internlm2-chat-1_8b-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2587, 'grad_norm': 4.242425441741943, 'learning_rate': 6.280400572246066e-06, 'epoch': 1.11}\n",
      "{'loss': 3.2858, 'grad_norm': 3.6935524940490723, 'learning_rate': 6.137339055793991e-06, 'epoch': 1.16}\n",
      "{'loss': 3.3091, 'grad_norm': 4.022952079772949, 'learning_rate': 5.994277539341917e-06, 'epoch': 1.2}\n",
      "{'loss': 3.365, 'grad_norm': 4.701659679412842, 'learning_rate': 5.851216022889843e-06, 'epoch': 1.24}\n",
      "{'loss': 3.2944, 'grad_norm': 3.604840040206909, 'learning_rate': 5.708154506437768e-06, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in ../models/internlm2-chat-1_8b-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2582, 'grad_norm': 3.769631862640381, 'learning_rate': 5.565092989985694e-06, 'epoch': 1.33}\n",
      "{'loss': 3.337, 'grad_norm': 3.633863687515259, 'learning_rate': 5.42203147353362e-06, 'epoch': 1.37}\n",
      "{'loss': 3.2657, 'grad_norm': 5.075555801391602, 'learning_rate': 5.2789699570815455e-06, 'epoch': 1.41}\n",
      "{'loss': 3.265, 'grad_norm': 3.8588244915008545, 'learning_rate': 5.135908440629471e-06, 'epoch': 1.46}\n",
      "{'loss': 3.4038, 'grad_norm': 4.402227401733398, 'learning_rate': 4.992846924177397e-06, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in ../models/internlm2-chat-1_8b-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2748, 'grad_norm': 3.8267390727996826, 'learning_rate': 4.849785407725322e-06, 'epoch': 1.54}\n",
      "{'loss': 3.2775, 'grad_norm': 4.4183807373046875, 'learning_rate': 4.7067238912732475e-06, 'epoch': 1.59}\n",
      "{'loss': 3.2974, 'grad_norm': 3.573885440826416, 'learning_rate': 4.563662374821173e-06, 'epoch': 1.63}\n",
      "{'loss': 3.284, 'grad_norm': 4.0890913009643555, 'learning_rate': 4.420600858369099e-06, 'epoch': 1.67}\n",
      "{'loss': 3.1742, 'grad_norm': 3.4867520332336426, 'learning_rate': 4.2775393419170246e-06, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in ../models/internlm2-chat-1_8b-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3819, 'grad_norm': 4.6432366371154785, 'learning_rate': 4.13447782546495e-06, 'epoch': 1.76}\n",
      "{'loss': 3.2152, 'grad_norm': 3.4291365146636963, 'learning_rate': 3.991416309012876e-06, 'epoch': 1.8}\n",
      "{'loss': 3.3147, 'grad_norm': 3.769305467605591, 'learning_rate': 3.848354792560802e-06, 'epoch': 1.84}\n",
      "{'loss': 3.2896, 'grad_norm': 5.2964701652526855, 'learning_rate': 3.705293276108727e-06, 'epoch': 1.89}\n",
      "{'loss': 3.2469, 'grad_norm': 4.24137544631958, 'learning_rate': 3.5622317596566526e-06, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in ../models/internlm2-chat-1_8b-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2712, 'grad_norm': 5.00827693939209, 'learning_rate': 3.4191702432045783e-06, 'epoch': 1.97}\n",
      "{'loss': 3.2031, 'grad_norm': 4.242588043212891, 'learning_rate': 3.2761087267525036e-06, 'epoch': 2.02}\n",
      "{'loss': 3.1839, 'grad_norm': 3.8554165363311768, 'learning_rate': 3.1330472103004293e-06, 'epoch': 2.06}\n",
      "{'loss': 3.2758, 'grad_norm': 4.536030292510986, 'learning_rate': 2.989985693848355e-06, 'epoch': 2.1}\n",
      "{'loss': 3.2532, 'grad_norm': 3.5911710262298584, 'learning_rate': 2.8469241773962807e-06, 'epoch': 2.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in ../models/internlm2-chat-1_8b-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.213, 'grad_norm': 4.78516149520874, 'learning_rate': 2.7038626609442064e-06, 'epoch': 2.19}\n",
      "{'loss': 3.2101, 'grad_norm': 5.027781009674072, 'learning_rate': 2.5608011444921317e-06, 'epoch': 2.23}\n",
      "{'loss': 3.2451, 'grad_norm': 4.482312202453613, 'learning_rate': 2.4177396280400574e-06, 'epoch': 2.27}\n",
      "{'loss': 3.2949, 'grad_norm': 4.7395853996276855, 'learning_rate': 2.274678111587983e-06, 'epoch': 2.32}\n",
      "{'loss': 3.3431, 'grad_norm': 5.098944664001465, 'learning_rate': 2.1316165951359088e-06, 'epoch': 2.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in ../models/internlm2-chat-1_8b-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3341, 'grad_norm': 4.268165111541748, 'learning_rate': 1.988555078683834e-06, 'epoch': 2.4}\n",
      "{'loss': 3.2716, 'grad_norm': 4.6500043869018555, 'learning_rate': 1.84549356223176e-06, 'epoch': 2.44}\n",
      "{'loss': 3.2109, 'grad_norm': 5.024342060089111, 'learning_rate': 1.7024320457796852e-06, 'epoch': 2.49}\n",
      "{'loss': 3.2639, 'grad_norm': 4.036920547485352, 'learning_rate': 1.559370529327611e-06, 'epoch': 2.53}\n",
      "{'loss': 3.1627, 'grad_norm': 4.781997203826904, 'learning_rate': 1.4163090128755366e-06, 'epoch': 2.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in ../models/internlm2-chat-1_8b-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2641, 'grad_norm': 3.7526755332946777, 'learning_rate': 1.2732474964234623e-06, 'epoch': 2.62}\n",
      "{'loss': 3.1777, 'grad_norm': 4.780666351318359, 'learning_rate': 1.1301859799713878e-06, 'epoch': 2.66}\n",
      "{'loss': 3.2875, 'grad_norm': 4.469741344451904, 'learning_rate': 9.871244635193133e-07, 'epoch': 2.7}\n",
      "{'loss': 3.2268, 'grad_norm': 4.040610313415527, 'learning_rate': 8.44062947067239e-07, 'epoch': 2.74}\n",
      "{'loss': 3.2484, 'grad_norm': 4.668625354766846, 'learning_rate': 7.010014306151645e-07, 'epoch': 2.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\peft\\utils\\save_and_load.py:154: UserWarning: Could not find a config file in ../models/internlm2-chat-1_8b-sft - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "d:\\miniconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3452, 'grad_norm': 5.963035583496094, 'learning_rate': 5.579399141630902e-07, 'epoch': 2.83}\n",
      "{'loss': 3.311, 'grad_norm': 4.150168418884277, 'learning_rate': 4.148783977110158e-07, 'epoch': 2.87}\n",
      "{'loss': 3.1485, 'grad_norm': 4.423460960388184, 'learning_rate': 2.718168812589414e-07, 'epoch': 2.92}\n",
      "{'loss': 3.217, 'grad_norm': 4.400463104248047, 'learning_rate': 1.2875536480686695e-07, 'epoch': 2.96}\n",
      "{'train_runtime': 2445.0788, 'train_samples_per_second': 4.575, 'train_steps_per_second': 0.286, 'train_loss': 3.4334965461654554, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=699, training_loss=3.4334965461654554, metrics={'train_runtime': 2445.0788, 'train_samples_per_second': 4.575, 'train_steps_per_second': 0.286, 'train_loss': 3.4334965461654554, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internlm-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
