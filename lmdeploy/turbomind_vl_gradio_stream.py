import os
from PIL import Image
import gradio as gr
from infer_engine import InferEngine, LmdeployConfig
from infer_utils import convert_to_multimodal_history
from typing import Generator, Sequence
import threading
from loguru import logger


logger.info(f"gradio version: {gr.__version__}")

USE_PIL = True

MODEL_PATH = '../models/InternVL2-2B'

SYSTEM_PROMPT = 'æˆ‘æ˜¯ä¹¦ç”ŸÂ·ä¸‡è±¡ï¼Œè‹±æ–‡åæ˜¯InternVLï¼Œæ˜¯ç”±ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤åŠå¤šå®¶åˆä½œå•ä½è”åˆå¼€å‘çš„å¤šæ¨¡æ€åŸºç¡€æ¨¡å‹ã€‚äººå·¥æ™ºèƒ½å®éªŒå®¤è‡´åŠ›äºåŸå§‹æŠ€æœ¯åˆ›æ–°ï¼Œå¼€æºå¼€æ”¾ï¼Œå…±äº«å…±åˆ›ï¼Œæ¨åŠ¨ç§‘æŠ€è¿›æ­¥å’Œäº§ä¸šå‘å±•ã€‚'


LMDEPLOY_CONFIG = LmdeployConfig(
    model_path = MODEL_PATH,
    backend = 'turbomind',
    model_name = 'internvl-internlm2',
    model_format = 'hf',
    tp = 1,                         # Tensor Parallelism.
    max_batch_size = 128,
    cache_max_entry_count= 0.8,     # è°ƒæ•´ KV Cache çš„å ç”¨æ¯”ä¾‹ä¸º0.8
    quant_policy = 0,               # KV Cache é‡åŒ–, 0 ä»£è¡¨ç¦ç”¨, 4 ä»£è¡¨ 4bit é‡åŒ–, 8 ä»£è¡¨ 8bit é‡åŒ–
    system_prompt = SYSTEM_PROMPT,
    deploy_method = 'local',
)

# è½½å…¥æ¨¡å‹
infer_engine = InferEngine(
    backend = 'lmdeploy', # transformers, lmdeploy, api
    lmdeploy_config = LMDEPLOY_CONFIG
)


class InterFace:
    global_session_id: int = 0
    lock = threading.Lock()


def multimodal_chat(
    query: dict,
    history: Sequence | None = None,  # [['What is the capital of France?', 'The capital of France is Paris.'], ['Thanks', 'You are Welcome']]
    max_new_tokens: int = 1024,
    temperature: float = 0.8,
    top_p: float = 0.8,
    top_k: int = 40,
    state_session_id: int = 0,
) -> Generator[Sequence, None, None]:
    history = [] if history is None else list(history)

    logger.info(f"{state_session_id = }")
    logger.info({
            "max_new_tokens":  max_new_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
    })

    logger.info(f"query: {query}")
    query_text = query["text"]
    # if query_text is None or len(query_text.strip()) == 0:
    if query_text is None or (len(query_text.strip()) == 0 and len(query["files"]) == 0):
        logger.warning(f"query is None, return history")
        yield history
        return
    query_text = query_text.strip()
    logger.info(f"query_text: {query_text}")
    # multimodal_query = query_text if len(query["files"]) <= 0 else (query_text, query["files"])
    multimodal_query = query_text if len(query["files"]) <= 0 else (
        query_text, [Image.open(file) if USE_PIL else file for file in query["files"]]
    ) # use pil
    logger.info(f"multimodal_query: {multimodal_query}")

    logger.info(f"history before: {history}")
    multimodal_history = convert_to_multimodal_history(history, use_pil=USE_PIL)
    logger.info(f"multimodal_history: {multimodal_history}")

    # å°†å›¾ç‰‡æ”¾å…¥å†å²è®°å½•ä¸­
    for file in query["files"]:
        history.append([(file,), None])

    yield history + [[query_text, None]]

    for response in infer_engine.chat_stream(
        query = multimodal_query,
        history = multimodal_history,
        max_new_tokens = 1024,
        temperature = 0.8,
        top_p = 0.8,
        top_k = 40,
        session_id = state_session_id,
    ):
        yield history + [[query_text, response]]
    logger.info(f"history after: {history + [[query_text, response]]}")


def regenerate(
    history: Sequence | None = None,  # [['What is the capital of France?', 'The capital of France is Paris.'], ['Thanks', 'You are Welcome']]
    max_new_tokens: int = 1024,
    temperature: float = 0.8,
    top_p: float = 0.8,
    top_k: int = 40,
    state_session_id: int = 0,
) -> Generator[Sequence, None, None]:
    history = [] if history is None else list(history)

    query = {'text': "", 'files': []}
    # é‡æ–°ç”Ÿæˆæ—¶è¦æŠŠæœ€åçš„queryå’Œresponseå¼¹å‡º,é‡ç”¨query
    if len(history) > 0:
        query_data, _ = history.pop(-1)
        if isinstance(query_data, str):
            query['text'] = query_data
        else:
            # è·å–æ–‡ä»¶
            query['files'].append(query_data[0])
        yield from multimodal_chat(
            query = query,
            history = history,
            max_new_tokens = max_new_tokens,
            temperature = temperature,
            top_p = top_p,
            top_k = top_k,
            state_session_id = state_session_id,
        )
    else:
        logger.warning(f"no history, can't regenerate")
        yield history


def revocery(query: dict, history: Sequence | None = None) -> tuple[str, Sequence]:
    """æ¢å¤åˆ°ä¸Šä¸€è½®å¯¹è¯"""
    history = [] if history is None else list(history)
    if len(history) > 0:
        query_data, _ = history.pop(-1)
        if isinstance(query_data, str):
            query['text'] = query_data
        else:
            # è·å–æ–‡ä»¶
            query['files'].append(query_data[0])
    return query, history


def main():
    block = gr.Blocks()
    with block as demo:
        state_session_id = gr.State(0)

        with gr.Row(equal_height=True):
            with gr.Column(scale=15):
                gr.Markdown("""<h1><center>ğŸ¦™ LLaMA 3</center></h1>
                    <center>ğŸ¦™ LLaMA 3 Chatbot ğŸ’¬</center>
                    """)
            # gr.Image(value=LOGO_PATH, scale=1, min_width=10,show_label=False, show_download_button=False)

        with gr.Row():
            with gr.Column(scale=4):
                with gr.Row():
                    # åˆ›å»ºèŠå¤©æ¡†
                    chatbot = gr.Chatbot(height=500, show_copy_button=True, placeholder="å†…å®¹ç”± AI å¤§æ¨¡å‹ç”Ÿæˆï¼Œè¯·ä»”ç»†ç”„åˆ«ã€‚")

                # ç»„å†…çš„ç»„ä»¶æ²¡æœ‰é—´è·
                with gr.Group():
                    with gr.Row():
                        # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                        query = gr.MultimodalTextbox(
                            file_types=["image"],
                            file_count='multiple', # æŒ‡çš„æ˜¯ä¸€æ¬¡ä¸Šä¼ å‡ å¼ ,é€‰æ‹©singleä¹Ÿå¯ä»¥å¤šæ¬¡é€‰æ‹©
                            placeholder="Enter å‘é€; Shift + Enter æ¢è¡Œ / Enter to send; Shift + Enter to wrap",
                            label="Prompt / é—®é¢˜",
                            interactive=True,
                        )

                with gr.Row():
                    # åˆ›å»ºä¸€ä¸ªé‡æ–°ç”ŸæˆæŒ‰é’®ï¼Œç”¨äºé‡æ–°ç”Ÿæˆå½“å‰å¯¹è¯å†…å®¹ã€‚
                    regen = gr.Button("ğŸ”„ Retry", variant="secondary")
                    undo = gr.Button("â†©ï¸ Undo", variant="secondary")
                    # åˆ›å»ºä¸€ä¸ªæ¸…é™¤æŒ‰é’®ï¼Œç”¨äºæ¸…é™¤èŠå¤©æœºå™¨äººç»„ä»¶çš„å†…å®¹ã€‚
                    clear = gr.ClearButton(components=[chatbot, query], value="ğŸ—‘ï¸ Clear", variant="stop")

                # æŠ˜å 
                with gr.Accordion("Advanced Options", open=False):
                    with gr.Row():
                        max_new_tokens = gr.Slider(
                            minimum=1,
                            maximum=2048,
                            value=1024,
                            step=1,
                            label='Max new tokens'
                        )
                        temperature = gr.Slider(
                            minimum=0.01,
                            maximum=2,
                            value=0.8,
                            step=0.01,
                            label='Temperature'
                        )
                        top_p = gr.Slider(
                            minimum=0.01,
                            maximum=1,
                            value=0.8,
                            step=0.01,
                            label='Top_p'
                        )
                        top_k = gr.Slider(
                            minimum=1,
                            maximum=100,
                            value=40,
                            step=1,
                            label='Top_k'
                        )

                gr.Examples(
                    examples=[
                        {'text': "ä½ æ˜¯è°", 'files': []},
                        {'text': "è¿™å¼ å›¾ç‰‡å±•ç¤ºçš„ä»€ä¹ˆå†…å®¹?", 'files': ['../images/0001.jpg']},
                        {'text': "è¿™2å¼ å›¾ç‰‡å±•ç¤ºçš„ä»€ä¹ˆå†…å®¹?", 'files': ['../images/0001.jpg', '../images/0002.jpg']},
                    ],
                    inputs=[query],
                    label="ç¤ºä¾‹é—®é¢˜ / Example questions"
                )

            # å›è½¦æäº¤
            query.submit(
                multimodal_chat,
                inputs=[query, chatbot, max_new_tokens, temperature, top_p, top_k, state_session_id],
                outputs=[chatbot]
            )

            # æ¸…ç©ºquery
            query.submit(
                lambda: gr.MultimodalTextbox(value={'text': "", 'files': []}),
                inputs=[],
                outputs=[query],
            )

            # é‡æ–°ç”Ÿæˆ
            regen.click(
                regenerate,
                inputs=[chatbot, max_new_tokens, temperature, top_p, top_k, state_session_id],
                outputs=[chatbot]
            )

            # æ’¤é”€
            undo.click(
                revocery,
                inputs=[query, chatbot],
                outputs=[query, chatbot]
            )

        gr.Markdown("""æé†’ï¼š<br>
        1. å†…å®¹ç”± AI å¤§æ¨¡å‹ç”Ÿæˆï¼Œè¯·ä»”ç»†ç”„åˆ«ã€‚<br>
        """)

        # åˆå§‹åŒ–session_id
        def init():
            with InterFace.lock:
                InterFace.global_session_id += 1
            new_session_id = InterFace.global_session_id
            return new_session_id

        demo.load(init, inputs=None, outputs=[state_session_id])

    # threads to consume the request
    gr.close_all()

    # è®¾ç½®é˜Ÿåˆ—å¯åŠ¨
    demo.queue(
        max_size = None,                # If None, the queue size will be unlimited.
        default_concurrency_limit = 100 # æœ€å¤§å¹¶å‘é™åˆ¶
    )

    # demo.launch(server_name = "127.0.0.1", server_port = 7860, share = True, max_threads = 100)
    demo.launch(max_threads = 100)


if __name__ == "__main__":
    main()
